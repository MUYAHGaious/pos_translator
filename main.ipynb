{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ee5591395e741e4b0706b5b6fd1ff43",
            "199ce35b59cc4a81a458018d63a7f957",
            "781843b95bc345089c1b60c252ac3e9b",
            "6984f959666243bd913e6effa32e2090",
            "8afeb2f39f614744950d7fb158e27405",
            "8c72d71c87024c9fa926e818506d3568",
            "8b6f86da8d294474831d0c9a486ed627",
            "d53bd732d21c44e2a2cfba5d8193ab24",
            "8435e5ecae5940739239f19b9a63186b",
            "29a5f0d89f4c4c3ca3ca705ed6cef77c",
            "cb0db1a91c5e4403978149a54e5f5d8b"
          ]
        },
        "id": "h-aUuaCgcXPE",
        "outputId": "788e2784-1de8-4301-a831-b520bfaddd16"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import datetime\n",
        "import psutil\n",
        "import multiprocessing\n",
        "import threading\n",
        "import concurrent.futures\n",
        "from tqdm.auto import tqdm\n",
        "from colorama import init, Fore, Style\n",
        "import re\n",
        "import gc\n",
        "import hashlib\n",
        "import shutil\n",
        "import ssl\n",
        "\n",
        "# Initialize colorama for colored terminal output\n",
        "init()\n",
        "\n",
        "# ====================\n",
        "# CONFIGURATION\n",
        "# ====================\n",
        "\n",
        "# Maximum words per POS category\n",
        "MAX_WORDS_PER_POS = 10000\n",
        "\n",
        "# Translation batch size - increased for better parallelism\n",
        "BATCH_SIZE = 500  \n",
        "\n",
        "# Target POS categories\n",
        "TARGET_POS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'DET', 'PRON', 'ADP', 'CONJ', 'NUM', 'INTJ']\n",
        "\n",
        "# Save interim results every N words\n",
        "INTERIM_SAVE_EVERY = 1000  \n",
        "\n",
        "# Multiprocessing settings - using more workers with available RAM\n",
        "CPU_COUNT = multiprocessing.cpu_count()\n",
        "MAX_WORKERS = min(CPU_COUNT * 2, 16)  # Scale workers based on available CPUs\n",
        "MAX_TRANSLATION_WORKERS = 8  # Dedicated workers for translation tasks\n",
        "\n",
        "# Translation model - Helsinki-NLP is a good alternative to NLLB\n",
        "PRIMARY_MODEL = \"facebook/nllb-200-distilled-1.3B\"  # Using larger model for better quality\n",
        "FALLBACK_MODEL = \"facebook/nllb-200-distilled-600M\"  # Fallback to smaller model if memory issues\n",
        "\n",
        "# Translation language codes\n",
        "LANGUAGE_CODES = {\n",
        "    \"english\": \"eng_Latn\", \n",
        "    \"french\": \"fra_Latn\", \n",
        "    \"bambara\": \"bam_Latn\", \n",
        "    \"wolof\": \"wol_Latn\"\n",
        "}\n",
        "\n",
        "# Custom NLTK data directory\n",
        "NLTK_DATA_DIR = os.path.expanduser(\"~/nltk_data\")\n",
        "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
        "nltk.data.path.append(NLTK_DATA_DIR)\n",
        "\n",
        "# POS tag mapping (for words not in WordNet)\n",
        "TAG_POS_MAP = {\n",
        "    'NN': 'NOUN', \n",
        "    'NNS': 'NOUN', \n",
        "    'NNP': 'NOUN', \n",
        "    'NNPS': 'NOUN',\n",
        "    'VB': 'VERB', \n",
        "    'VBD': 'VERB', \n",
        "    'VBG': 'VERB', \n",
        "    'VBN': 'VERB', \n",
        "    'VBP': 'VERB', \n",
        "    'VBZ': 'VERB',\n",
        "    'JJ': 'ADJ', \n",
        "    'JJR': 'ADJ', \n",
        "    'JJS': 'ADJ',\n",
        "    'RB': 'ADV', \n",
        "    'RBR': 'ADV', \n",
        "    'RBS': 'ADV',\n",
        "    'DT': 'DET', \n",
        "    'PDT': 'DET', \n",
        "    'WDT': 'DET',\n",
        "    'PRP': 'PRON', \n",
        "    'PRPS': 'PRON',  # Using PRPS instead of PRP$ to avoid $ character\n",
        "    'WP': 'PRON', \n",
        "    'WPS': 'PRON',   # Using WPS instead of WP$ to avoid $ character\n",
        "    'IN': 'ADP',\n",
        "    'CC': 'CONJ',\n",
        "    'CD': 'NUM',\n",
        "    'UH': 'INTJ'\n",
        "}\n",
        "\n",
        "# WN_POS_MAP will be configured at runtime based on WordNet availability\n",
        "WN_POS_MAP = {}\n",
        "\n",
        "# ====================\n",
        "# DIRECTORY SETUP\n",
        "# ====================\n",
        "\n",
        "def setup_drive_directories():\n",
        "    \"\"\"Set up directory structure for the project\"\"\"\n",
        "    try:\n",
        "        # Try importing Google Colab drive module\n",
        "        from google.colab import drive\n",
        "        \n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        # Main project directory\n",
        "        base_dir = \"/content/drive/MyDrive/multilingual_dataset\"\n",
        "        os.makedirs(base_dir, exist_ok=True)\n",
        "        \n",
        "        print(f\"{Fore.GREEN}✅ Google Drive mounted successfully{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.YELLOW}⚠️ Not running in Google Colab or drive access error: {str(e)}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.YELLOW}⚠️ Using local directories instead{Style.RESET_ALL}\")\n",
        "        \n",
        "        # Create local base directory\n",
        "        base_dir = \"./multilingual_dataset\"\n",
        "        os.makedirs(base_dir, exist_ok=True)\n",
        "    \n",
        "    # Create subdirectories\n",
        "    directories = {\n",
        "        \"data\": os.path.join(base_dir, \"data\"),\n",
        "        \"pos_data\": os.path.join(base_dir, \"data/pos\"),\n",
        "        \"output\": os.path.join(base_dir, \"output\"),\n",
        "        \"checkpoints\": os.path.join(base_dir, \"checkpoints\"),\n",
        "        \"logs\": os.path.join(base_dir, \"logs\"),\n",
        "        \"interim\": os.path.join(base_dir, \"interim_results\"),\n",
        "        \"cache\": os.path.join(base_dir, \"cache\"),\n",
        "        \"samples\": os.path.join(base_dir, \"samples\"),\n",
        "        \"translations\": os.path.join(base_dir, \"translations\"),\n",
        "        \"final\": os.path.join(base_dir, \"final_results\")\n",
        "    }\n",
        "    \n",
        "    # Create all directories\n",
        "    for path in directories.values():\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "    print(f\"{Fore.GREEN}✅ Created all project directories{Style.RESET_ALL}\")\n",
        "    return base_dir, directories\n",
        "\n",
        "# ====================\n",
        "# LOGGING & MONITORING\n",
        "# ====================\n",
        "\n",
        "def get_timestamp():\n",
        "    \"\"\"Get current timestamp for logging\"\"\"\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "class Logger:\n",
        "    \"\"\"Enhanced logger with structured logging and progress tracking\"\"\"\n",
        "    def __init__(self, log_dir):\n",
        "        self.log_dir = log_dir\n",
        "        self.main_log = os.path.join(log_dir, \"main.log\")\n",
        "        self.progress_log = os.path.join(log_dir, \"progress.log\")\n",
        "        self.completed_log = os.path.join(log_dir, \"completed_steps.log\")\n",
        "        self.error_log = os.path.join(log_dir, \"errors.log\")\n",
        "        \n",
        "        # Create log files if they don't exist\n",
        "        for log_file in [self.main_log, self.progress_log, self.completed_log, self.error_log]:\n",
        "            if not os.path.exists(log_file):\n",
        "                with open(log_file, 'w') as f:\n",
        "                    f.write(f\"Log started at {get_timestamp()}\\n\")\n",
        "        \n",
        "        # Track completed steps\n",
        "        self.completed_steps = self._load_completed_steps()\n",
        "        \n",
        "        # Set up log-specific locks to prevent race conditions in multi-threading\n",
        "        self.main_log_lock = threading.Lock()\n",
        "        self.progress_log_lock = threading.Lock()\n",
        "        self.completed_log_lock = threading.Lock()\n",
        "        self.error_log_lock = threading.Lock()\n",
        "    \n",
        "    def _load_completed_steps(self):\n",
        "        \"\"\"Load list of completed steps from log file\"\"\"\n",
        "        completed = set()\n",
        "        if os.path.exists(self.completed_log):\n",
        "            with open(self.completed_log, 'r') as f:\n",
        "                for line in f:\n",
        "                    if ',' in line:\n",
        "                        step = line.strip().split(',')[0]\n",
        "                        completed.add(step)\n",
        "        return completed\n",
        "    \n",
        "    def log(self, message, level=\"INFO\"):\n",
        "        \"\"\"Log a message to the main log file\"\"\"\n",
        "        timestamp = get_timestamp()\n",
        "        color = Fore.GREEN if level == \"INFO\" else Fore.YELLOW if level == \"WARNING\" else Fore.RED\n",
        "        log_message = f\"[{timestamp}] {level}: {message}\"\n",
        "        \n",
        "        # Print to console\n",
        "        print(f\"{color}{log_message}{Style.RESET_ALL}\")\n",
        "        \n",
        "        # Write to log file\n",
        "        with self.main_log_lock:\n",
        "            with open(self.main_log, 'a') as f:\n",
        "                f.write(f\"{log_message}\\n\")\n",
        "                \n",
        "        # Write to error log if error\n",
        "        if level == \"ERROR\":\n",
        "            with self.error_log_lock:\n",
        "                with open(self.error_log, 'a') as f:\n",
        "                    f.write(f\"{log_message}\\n\")\n",
        "    \n",
        "    def track_progress(self, phase, current, total, additional_info=\"\"):\n",
        "        \"\"\"Track progress of a specific phase\"\"\"\n",
        "        timestamp = get_timestamp()\n",
        "        percentage = (current / total) * 100 if total > 0 else 0\n",
        "        progress_message = f\"[{timestamp}] {phase}: {current}/{total} ({percentage:.2f}%) {additional_info}\"\n",
        "        \n",
        "        # Write to progress log\n",
        "        with self.progress_log_lock:\n",
        "            with open(self.progress_log, 'a') as f:\n",
        "                f.write(f\"{progress_message}\\n\")\n",
        "    \n",
        "    def complete_step(self, step_name):\n",
        "        \"\"\"Mark a step as completed\"\"\"\n",
        "        timestamp = get_timestamp()\n",
        "        \n",
        "        with self.completed_log_lock:\n",
        "            with open(self.completed_log, 'a') as f:\n",
        "                f.write(f\"{step_name},{timestamp}\\n\")\n",
        "            \n",
        "        self.completed_steps.add(step_name)\n",
        "        self.log(f\"✅ Completed step: {step_name}\")\n",
        "    \n",
        "    def is_completed(self, step_name):\n",
        "        \"\"\"Check if a step is already completed\"\"\"\n",
        "        return step_name in self.completed_steps\n",
        "    \n",
        "    def summary(self):\n",
        "        \"\"\"Print a summary of completed steps\"\"\"\n",
        "        self.log(\"=\" * 50)\n",
        "        self.log(\"SUMMARY OF COMPLETED STEPS\")\n",
        "        self.log(\"=\" * 50)\n",
        "        \n",
        "        for step in sorted(self.completed_steps):\n",
        "            self.log(f\"✓ {step}\")\n",
        "        \n",
        "        self.log(\"=\" * 50)\n",
        "\n",
        "# System monitoring\n",
        "def display_system_info():\n",
        "    \"\"\"Display detailed system info for monitoring resources\"\"\"\n",
        "    try:\n",
        "        # Get memory usage\n",
        "        memory = psutil.virtual_memory()\n",
        "        memory_used_gb = memory.used / (1024 ** 3)\n",
        "        memory_total_gb = memory.total / (1024 ** 3)\n",
        "        memory_percent = memory.percent\n",
        "        \n",
        "        # Get swap usage\n",
        "        swap = psutil.swap_memory()\n",
        "        swap_used_gb = swap.used / (1024 ** 3)\n",
        "        swap_total_gb = swap.total / (1024 ** 3)\n",
        "        swap_percent = swap.percent\n",
        "        \n",
        "        # Get CPU usage\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "        cpu_freq = psutil.cpu_freq()\n",
        "        cpu_freq_str = f\"{cpu_freq.current:.2f} MHz\" if cpu_freq else \"N/A\"\n",
        "        \n",
        "        # Get disk usage\n",
        "        disk = psutil.disk_usage('/')\n",
        "        disk_used_gb = disk.used / (1024 ** 3)\n",
        "        disk_total_gb = disk.total / (1024 ** 3)\n",
        "        disk_percent = disk.percent\n",
        "        \n",
        "        # Get GPU info if available\n",
        "        gpu_info = \"\"\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else \"N/A\"\n",
        "            gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "            gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
        "            gpu_info = f\"GPU: {gpu_name} ({gpu_count} devices)\\n\"\n",
        "            gpu_info += f\"GPU Memory: {gpu_memory_allocated:.2f}GB allocated / {gpu_memory_reserved:.2f}GB reserved\"\n",
        "        \n",
        "        # Print system info\n",
        "        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}SYSTEM INFORMATION{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n",
        "        print(f\"CPU Usage: {cpu_percent}% (Freq: {cpu_freq_str})\")\n",
        "        print(f\"RAM: {memory_used_gb:.2f}GB / {memory_total_gb:.2f}GB ({memory_percent}%)\")\n",
        "        print(f\"Swap: {swap_used_gb:.2f}GB / {swap_total_gb:.2f}GB ({swap_percent}%)\")\n",
        "        print(f\"Disk: {disk_used_gb:.2f}GB / {disk_total_gb:.2f}GB ({disk_percent}%)\")\n",
        "        if gpu_info:\n",
        "            print(gpu_info)\n",
        "        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.YELLOW}⚠️ Couldn't display system info: {str(e)}{Style.RESET_ALL}\")\n",
        "\n",
        "# Cache manager for expensive operations\n",
        "class CacheManager:\n",
        "    \"\"\"Manage caching of expensive operations\"\"\"\n",
        "    def __init__(self, cache_dir):\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.cache_lock = threading.Lock()\n",
        "    \n",
        "    def get_cache_path(self, key, category):\n",
        "        \"\"\"Get path for a cached item\"\"\"\n",
        "        # Create a hash of the key for filename\n",
        "        key_hash = hashlib.md5(str(key).encode()).hexdigest()\n",
        "        category_dir = os.path.join(self.cache_dir, category)\n",
        "        os.makedirs(category_dir, exist_ok=True)\n",
        "        return os.path.join(category_dir, f\"{key_hash}.json\")\n",
        "    \n",
        "    def get(self, key, category):\n",
        "        \"\"\"Get item from cache\"\"\"\n",
        "        cache_path = self.get_cache_path(key, category)\n",
        "        if os.path.exists(cache_path):\n",
        "            try:\n",
        "                with open(cache_path, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "    \n",
        "    def set(self, key, value, category):\n",
        "        \"\"\"Store item in cache\"\"\"\n",
        "        cache_path = self.get_cache_path(key, category)\n",
        "        with self.cache_lock:\n",
        "            with open(cache_path, 'w') as f:\n",
        "                json.dump(value, f, ensure_ascii=False)\n",
        "    \n",
        "    def clear_category(self, category):\n",
        "        \"\"\"Clear all cached items in a category\"\"\"\n",
        "        category_dir = os.path.join(self.cache_dir, category)\n",
        "        if os.path.exists(category_dir):\n",
        "            shutil.rmtree(category_dir)\n",
        "            os.makedirs(category_dir, exist_ok=True)\n",
        "\n",
        "# ====================\n",
        "# DATA EXTRACTION\n",
        "# ====================\n",
        "\n",
        "def download_nltk_data():\n",
        "    \"\"\"Download required NLTK data with proper SSL handling and verification\"\"\"\n",
        "    print(f\"{Fore.CYAN}Downloading NLTK resources...{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Fix SSL issues that might prevent downloads\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "    \n",
        "    # Resources to download\n",
        "    resources = [\n",
        "        'wordnet',\n",
        "        'omw-1.4',  # Open Multilingual WordNet\n",
        "        'words',\n",
        "        'averaged_perceptron_tagger',\n",
        "        'punkt',\n",
        "        'tagsets',\n",
        "        'universal_tagset'\n",
        "    ]\n",
        "    \n",
        "    # Download each resource with explicit console output\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            print(f\"Downloading {resource}... \", end=\"\", flush=True)\n",
        "            download_result = nltk.download(resource)\n",
        "            if download_result:\n",
        "                print(f\"{Fore.GREEN}Success{Style.RESET_ALL}\")\n",
        "            else:\n",
        "                print(f\"{Fore.RED}Failed{Style.RESET_ALL}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{Fore.RED}Error: {str(e)}{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Verification phase\n",
        "    print(f\"\\n{Fore.CYAN}Verifying critical NLTK resources:{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Test WordNet\n",
        "    wordnet_ok = False\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "        synsets = wn.synsets('test')\n",
        "        if synsets:\n",
        "            print(f\"{Fore.GREEN}✓ WordNet is working{Style.RESET_ALL}\")\n",
        "            wordnet_ok = True\n",
        "        else:\n",
        "            print(f\"{Fore.YELLOW}? WordNet available but returned no data{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.RED}✗ WordNet failed: {str(e)}{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Test words corpus\n",
        "    words_ok = False\n",
        "    try:\n",
        "        from nltk.corpus import words\n",
        "        word_list = words.words()\n",
        "        if word_list:\n",
        "            print(f\"{Fore.GREEN}✓ Words corpus is working ({len(word_list)} words){Style.RESET_ALL}\")\n",
        "            words_ok = True\n",
        "        else:\n",
        "            print(f\"{Fore.YELLOW}? Words corpus available but returned no data{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.RED}✗ Words corpus failed: {str(e)}{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Test POS tagger\n",
        "    tagger_ok = False\n",
        "    try:\n",
        "        from nltk.tag import pos_tag\n",
        "        tagged = pos_tag(['test', 'this'])\n",
        "        if tagged:\n",
        "            print(f\"{Fore.GREEN}✓ POS tagger is working{Style.RESET_ALL}\")\n",
        "            tagger_ok = True\n",
        "        else:\n",
        "            print(f\"{Fore.YELLOW}? POS tagger available but returned no data{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.RED}✗ POS tagger failed: {str(e)}{Style.RESET_ALL}\")\n",
        "    \n",
        "    # Return status of critical components\n",
        "    return {\"wordnet\": wordnet_ok, \"words\": words_ok, \"tagger\": tagger_ok}\n",
        "\n",
        "def process_wordnet_pos(wn_pos, target_pos, existing_words, words_per_pos, max_words):\n",
        "    \"\"\"Process a specific WordNet POS category - for parallel processing\"\"\"\n",
        "    if words_per_pos.get(target_pos, 0) >= max_words:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "        synsets = list(wn.all_synsets(wn_pos))\n",
        "        random.shuffle(synsets)  # Randomize to get a diverse set\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.RED}Error accessing WordNet: {str(e)}{Style.RESET_ALL}\")\n",
        "        # Return empty list if WordNet access fails\n",
        "        return []\n",
        "\n",
        "    # Process synsets\n",
        "    new_rows = []\n",
        "    needed_words = max_words - words_per_pos.get(target_pos, 0)\n",
        "    \n",
        "    for synset in synsets:\n",
        "        for lemma in synset.lemma_names():\n",
        "            # Skip multi-word expressions and non-alphabetic words\n",
        "            if '_' in lemma or not lemma.isalpha() or len(lemma) <= 2:\n",
        "                continue\n",
        "\n",
        "            lemma = lemma.lower()\n",
        "            if (lemma, target_pos) not in existing_words:\n",
        "                new_row = {\"word\": lemma, \"pos\": target_pos}\n",
        "                new_rows.append(new_row)\n",
        "                existing_words.add((lemma, target_pos))\n",
        "\n",
        "                # Check if we have enough words for this POS\n",
        "                if len(new_rows) >= needed_words:\n",
        "                    return new_rows\n",
        "\n",
        "    return new_rows\n",
        "\n",
        "def extract_lexicon_from_nltk_words(words_df, existing_words, words_per_pos, max_words, logger, num_workers=MAX_WORKERS):\n",
        "    \"\"\"Extract words from NLTK's word list with parallel tagging\"\"\"\n",
        "    try:\n",
        "        from nltk.corpus import words\n",
        "        \n",
        "        english_words = words.words()\n",
        "        random.shuffle(english_words)  # Randomize the list\n",
        "        \n",
        "        logger.log(f\"Processing {len(english_words[:100000])} words from NLTK corpus\")\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Error loading NLTK words corpus: {str(e)}\", \"ERROR\")\n",
        "        logger.log(\"Skipping NLTK word extraction\", \"WARNING\")\n",
        "        return words_df, 0\n",
        "    \n",
        "    # Dictionary to track words needed by POS\n",
        "    needed_by_pos = {pos: max(0, max_words - words_per_pos.get(pos, 0)) for pos in TARGET_POS}\n",
        "    total_needed = sum(needed_by_pos.values())\n",
        "    \n",
        "    if total_needed <= 0:\n",
        "        logger.log(\"No additional words needed from NLTK corpus\")\n",
        "        return words_df, 0\n",
        "    \n",
        "    logger.log(f\"Need {total_needed} more words from NLTK corpus\")\n",
        "    \n",
        "    try:\n",
        "        # Import NLTK POS tagger here to ensure it's available\n",
        "        from nltk.tag import pos_tag\n",
        "        \n",
        "        # Process in chunks for better parallelization\n",
        "        chunk_size = 5000  # Larger chunks for better parallelization\n",
        "        word_batches = [english_words[i:i+chunk_size] for i in range(0, min(100000, len(english_words)), chunk_size)]\n",
        "        \n",
        "        # New words by POS\n",
        "        new_words_by_pos = {pos: [] for pos in TARGET_POS}\n",
        "        tagged_count = 0\n",
        "        \n",
        "        # Process batches in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            # Map each batch to a tagging task\n",
        "            future_to_batch = {executor.submit(pos_tag, batch): batch for batch in word_batches}\n",
        "            \n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_batch), \n",
        "                             total=len(future_to_batch), \n",
        "                             desc=\"Tagging word batches\"):\n",
        "                try:\n",
        "                    tagged_words = future.result()\n",
        "                    tagged_count += len(tagged_words)\n",
        "                    \n",
        "                    # Process tagged words\n",
        "                    for word, tag in tagged_words:\n",
        "                        if not word.isalpha() or len(word) <= 2:\n",
        "                            continue\n",
        "                        \n",
        "                        word = word.lower()\n",
        "                        pos = TAG_POS_MAP.get(tag)\n",
        "                        \n",
        "                        if pos in TARGET_POS and needed_by_pos[pos] > 0 and (word, pos) not in existing_words:\n",
        "                            new_words_by_pos[pos].append({\"word\": word, \"pos\": pos})\n",
        "                            existing_words.add((word, pos))\n",
        "                            needed_by_pos[pos] -= 1\n",
        "                            \n",
        "                    # Check if we have all needed words\n",
        "                    total_remaining = sum(needed_by_pos.values())\n",
        "                    logger.track_progress(\"NLTK words processed\", tagged_count, 100000, \n",
        "                                        f\"Words remaining: {total_remaining}\")\n",
        "                    \n",
        "                    if total_remaining <= 0:\n",
        "                        logger.log(\"Collected all needed words from NLTK corpus\")\n",
        "                        break\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    logger.log(f\"Error processing batch: {str(e)}\", \"ERROR\")\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Error in NLTK POS tagging: {str(e)}\", \"ERROR\")\n",
        "        logger.log(\"Will continue with other word sources\", \"WARNING\")\n",
        "        return words_df, 0\n",
        "    \n",
        "    # Add new words to dataframe\n",
        "    total_added = 0\n",
        "    for pos in TARGET_POS:\n",
        "        if new_words_by_pos[pos]:\n",
        "            new_df = pd.DataFrame(new_words_by_pos[pos])\n",
        "            words_df = pd.concat([words_df, new_df], ignore_index=True)\n",
        "            total_added += len(new_words_by_pos[pos])\n",
        "            logger.log(f\"Added {len(new_words_by_pos[pos])} {pos} words from NLTK corpus\")\n",
        "    \n",
        "    return words_df, total_added\n",
        "\n",
        "def extract_words_with_pos_tags(directories, logger, cache_manager):\n",
        "    \"\"\"Extract words with POS tags using multiple sources with parallel processing\"\"\"\n",
        "    output_file = os.path.join(directories[\"pos_data\"], \"english_words.csv\")\n",
        "    checkpoint_file = os.path.join(directories[\"checkpoints\"], \"words_extraction_checkpoint.csv\")\n",
        "    \n",
        "    # Skip if already completed\n",
        "    if logger.is_completed(\"words_extraction\") and os.path.exists(output_file):\n",
        "        logger.log(\"Skipping word extraction - already completed\")\n",
        "        return pd.read_csv(output_file)\n",
        "    \n",
        "    # Check if resuming from checkpoint\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        words_df = pd.read_csv(checkpoint_file)\n",
        "        logger.log(f\"Resuming from checkpoint with {len(words_df)} entries\")\n",
        "    else:\n",
        "        # Initialize DataFrame\n",
        "        words_df = pd.DataFrame(columns=[\"word\", \"pos\"])\n",
        "    \n",
        "    # Track words we've already added\n",
        "    existing_words = set((row['word'], row['pos']) for _, row in words_df.iterrows())\n",
        "    words_per_pos = {}\n",
        "    \n",
        "    for pos in TARGET_POS:\n",
        "        pos_words = words_df[words_df['pos'] == pos]\n",
        "        words_per_pos[pos] = len(pos_words)\n",
        "        logger.log(f\"Current count for {pos}: {words_per_pos[pos]}/{MAX_WORDS_PER_POS} words\")\n",
        "    \n",
        "    # Display system info\n",
        "    display_system_info()\n",
        "    \n",
        "    # Import WordNet inside the function to ensure it's loaded after download\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn\n",
        "        # Define WordNet POS mapping inside function to use the loaded module\n",
        "        wn_pos_map = {\n",
        "            wn.NOUN: 'NOUN',\n",
        "            wn.VERB: 'VERB',\n",
        "            wn.ADJ: 'ADJ',\n",
        "            wn.ADV: 'ADV'\n",
        "        }\n",
        "        logger.log(\"Successfully imported WordNet\")\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Error importing WordNet: {str(e)}\", \"ERROR\")\n",
        "        # Use simplified extraction without WordNet if it fails\n",
        "        wn_pos_map = {}\n",
        "        logger.log(\"Will use alternative methods for word extraction\", \"WARNING\")\n",
        "    \n",
        "    # PHASE 1: Extract from WordNet (parallel processing)\n",
        "    if wn_pos_map:  # Only if WordNet loaded successfully\n",
        "        logger.log(\"Starting WordNet extraction with parallel processing...\")\n",
        "        \n",
        "        # Filter out POS categories that already have enough words\n",
        "        pos_to_process = {wn_pos: target_pos for wn_pos, target_pos in wn_pos_map.items() \n",
        "                        if words_per_pos.get(target_pos, 0) < MAX_WORDS_PER_POS}\n",
        "        \n",
        "        if pos_to_process:\n",
        "            logger.log(f\"Processing {len(pos_to_process)} POS categories from WordNet\")\n",
        "            \n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "                # Submit tasks for each POS category\n",
        "                future_to_pos = {\n",
        "                    executor.submit(\n",
        "                        process_wordnet_pos, \n",
        "                        wn_pos, \n",
        "                        target_pos, \n",
        "                        existing_words, \n",
        "                        words_per_pos,\n",
        "                        MAX_WORDS_PER_POS\n",
        "                    ): target_pos \n",
        "                    for wn_pos, target_pos in pos_to_process.items()\n",
        "                }\n",
        "                \n",
        "                # Process results as they come in\n",
        "                for future in concurrent.futures.as_completed(future_to_pos):\n",
        "                    target_pos = future_to_pos[future]\n",
        "                    try:\n",
        "                        new_rows = future.result()\n",
        "                        if new_rows:\n",
        "                            # Create DataFrame for new words\n",
        "                            new_df = pd.DataFrame(new_rows)\n",
        "                            \n",
        "                            # Save checkpoint for this POS category\n",
        "                            pos_checkpoint = os.path.join(directories[\"checkpoints\"], f\"{target_pos}_wordnet.csv\")\n",
        "                            new_df.to_csv(pos_checkpoint, index=False)\n",
        "                            \n",
        "                            # Update master DataFrame\n",
        "                            words_df = pd.concat([words_df, new_df], ignore_index=True)\n",
        "                            \n",
        "                            # Save intermediate DataFrame to the checkpoint file\n",
        "                            words_df.to_csv(checkpoint_file, index=False)\n",
        "                            \n",
        "                            # Update counts\n",
        "                            words_per_pos[target_pos] = len(words_df[words_df['pos'] == target_pos])\n",
        "                            \n",
        "                            logger.log(f\"Added {len(new_rows)} {target_pos} words from WordNet\")\n",
        "                            logger.log(f\"Current count for {target_pos}: {words_per_pos[target_pos]}/{MAX_WORDS_PER_POS}\")\n",
        "                    except Exception as e:\n",
        "                        logger.log(f\"Error processing {target_pos} from WordNet: {str(e)}\", \"ERROR\")\n",
        "    else:\n",
        "        logger.log(\"Skipping WordNet extraction due to import error\", \"WARNING\")\n",
        "    \n",
        "    # PHASE 2: Add common, high-quality words for certain categories\n",
        "    logger.log(\"Adding high-quality common words for specific categories...\")\n",
        "    \n",
        "    # Common words dictionary with curated high-quality translations\n",
        "    common_words = {\n",
        "        'DET': ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'our', 'their', 'its',\n",
        "                'each', 'every', 'some', 'any', 'no', 'all', 'both', 'either', 'neither', 'many', 'much', 'most', 'few',\n",
        "                'which', 'what', 'whose', 'whichever', 'whatever'],\n",
        "        'PRON': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'who', 'whom', 'which',\n",
        "                'what', 'whose', 'whoever', 'whomever', 'whatever', 'myself', 'yourself', 'himself', 'herself', 'itself',\n",
        "                'ourselves', 'yourselves', 'themselves', 'each', 'other', 'all', 'another', 'some', 'any', 'somebody',\n",
        "                'anyone', 'everybody', 'nobody', 'something', 'anything', 'nothing', 'everything'],\n",
        "        'ADP': ['in', 'on', 'at', 'by', 'with', 'from', 'to', 'for', 'of', 'about', 'against', 'between', 'through',\n",
        "                'during', 'before', 'after', 'above', 'below', 'under', 'over', 'beside', 'behind', 'across', 'into',\n",
        "                'towards', 'onto', 'beyond', 'along', 'amid', 'among', 'around', 'concerning', 'considering', 'despite',\n",
        "                'except', 'inside', 'like', 'near', 'off', 'out', 'outside', 'past', 'regarding', 'round', 'since',\n",
        "                'throughout', 'till', 'until', 'upon', 'within', 'without'],\n",
        "        'CONJ': ['and', 'or', 'but', 'if', 'when', 'than', 'because', 'although', 'since', 'unless', 'while',\n",
        "                'as', 'that', 'whether', 'before', 'after', 'though', 'so', 'till', 'until', 'whereas', 'for',\n",
        "                'nor', 'yet', 'once', 'provided', 'supposing', 'considering', 'even', 'otherwise', 'however'],\n",
        "        'NUM': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'first', 'second',\n",
        "                'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'once', 'twice', 'thrice',\n",
        "                'dozen', 'hundred', 'thousand', 'million', 'billion', 'trillion', 'zero', 'half', 'quarter', 'double',\n",
        "                'triple', 'quadruple', 'many', 'few', 'several', 'numerous', 'countless'],\n",
        "        'INTJ': ['oh', 'wow', 'hey', 'hello', 'hi', 'yes', 'no', 'okay', 'well', 'ah', 'ouch', 'ugh', 'oops',\n",
        "                'thanks', 'sorry', 'please', 'goodbye', 'bye', 'eh', 'hmm', 'er', 'um', 'alas', 'hurray', 'hooray',\n",
        "                'bravo', 'congratulations', 'cheers', 'yay', 'yikes', 'phew', 'whew', 'huh', 'damn', 'darn', 'gosh',\n",
        "                'goodness', 'jeez', 'bingo', 'voila', 'encore', 'oops', 'psst', 'shh', 'whoa', 'ha', 'haha']\n",
        "    }\n",
        "    \n",
        "    for pos, words_list in common_words.items():\n",
        "        added_count = 0\n",
        "        for word in words_list:\n",
        "            if (word, pos) not in existing_words:\n",
        "                # Add to DataFrame\n",
        "                new_row = pd.DataFrame([{\"word\": word, \"pos\": pos}])\n",
        "                words_df = pd.concat([words_df, new_row], ignore_index=True)\n",
        "                \n",
        "                # Update tracking sets\n",
        "                existing_words.add((word, pos))\n",
        "                added_count += 1\n",
        "        \n",
        "        if added_count > 0:\n",
        "            logger.log(f\"Added {added_count} common words for {pos}\")\n",
        "            words_per_pos[pos] = len(words_df[words_df['pos'] == pos])\n",
        "    \n",
        "    # Save checkpoint\n",
        "    words_df.to_csv(checkpoint_file, index=False)\n",
        "    \n",
        "    # PHASE 3: Use NLTK word list for additional words\n",
        "    logger.log(\"Extracting additional words from NLTK corpus...\")\n",
        "    \n",
        "    words_df, nltk_words_added = extract_lexicon_from_nltk_words(\n",
        "        words_df, existing_words, words_per_pos, MAX_WORDS_PER_POS, logger, MAX_WORKERS\n",
        "    )\n",
        "    \n",
        "    if nltk_words_added > 0:\n",
        "        logger.log(f\"Added {nltk_words_added} words from NLTK corpus\")\n",
        "        words_df.to_csv(checkpoint_file, index=False)\n",
        "    \n",
        "    # PHASE 4: Fill remaining categories with synthetic words if needed\n",
        "    logger.log(\"Checking if synthetic words are needed...\")\n",
        "    \n",
        "    def generate_synthetic_words(pos, needed_count):\n",
        "        \"\"\"Generate synthetic words for a POS category\"\"\"\n",
        "        synthetic_words = []\n",
        "        for i in range(1, needed_count + 1):\n",
        "            word = f\"{pos.lower()}{i:05d}\"  # Format with leading zeros for consistent sorting\n",
        "            synthetic_words.append({\"word\": word, \"pos\": pos})\n",
        "        return synthetic_words\n",
        "    \n",
        "    # Check each POS category and add synthetic words if needed\n",
        "    for pos in TARGET_POS:\n",
        "        current_count = len(words_df[words_df['pos'] == pos])\n",
        "        if current_count < MAX_WORDS_PER_POS:\n",
        "            needed = MAX_WORDS_PER_POS - current_count\n",
        "            logger.log(f\"Generating {needed} synthetic words for {pos}\")\n",
        "            \n",
        "            synthetic_df = pd.DataFrame(generate_synthetic_words(pos, needed))\n",
        "            words_df = pd.concat([words_df, synthetic_df], ignore_index=True)\n",
        "            \n",
        "            logger.log(f\"Added {needed} synthetic words for {pos}\")\n",
        "    \n",
        "    # Final counts\n",
        "    for pos in TARGET_POS:\n",
        "        count = len(words_df[words_df['pos'] == pos])\n",
        "        logger.log(f\"Final count for {pos}: {count}/{MAX_WORDS_PER_POS} words\")\n",
        "    \n",
        "    # Save final output\n",
        "    words_df.to_csv(output_file, index=False)\n",
        "    logger.log(f\"Saved {len(words_df)} words with POS tags to {output_file}\")\n",
        "    \n",
        "    # Mark step as completed\n",
        "    logger.complete_step(\"words_extraction\")\n",
        "    \n",
        "    return words_df\n",
        "\n",
        "# ====================\n",
        "# TRANSLATION\n",
        "# ====================\n",
        "\n",
        "def setup_translation_model(logger, cache_dir):\n",
        "    \"\"\"Set up translation model with proper memory optimization\"\"\"\n",
        "    try:\n",
        "        # Import transformers here to ensure it's available\n",
        "        from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "        \n",
        "        # Choose model based on available memory\n",
        "        model_name = PRIMARY_MODEL\n",
        "        logger.log(f\"Loading translation model: {model_name}\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        # Configure model with memory optimizations\n",
        "        if torch.cuda.is_available():\n",
        "            # GPU optimizations\n",
        "            logger.log(\"Using GPU for translation with mixed precision\")\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16,  # Use half precision\n",
        "                device_map=\"auto\",          # Auto-distribute across GPUs\n",
        "                low_cpu_mem_usage=True,     # Reduce CPU memory usage\n",
        "                cache_dir=cache_dir         # Cache model files\n",
        "            )\n",
        "        else:\n",
        "            # CPU optimizations\n",
        "            logger.log(\"Using CPU for translation\")\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                model_name,\n",
        "                low_cpu_mem_usage=True,     # Reduce memory usage\n",
        "                cache_dir=cache_dir         # Cache model files\n",
        "            )\n",
        "        \n",
        "        # Create pipeline\n",
        "        translator = pipeline(\n",
        "            \"translation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1,\n",
        "            batch_size=BATCH_SIZE  # Adjust batch size for better throughput\n",
        "        )\n",
        "        \n",
        "        logger.log(f\"Successfully loaded translation model: {model_name}\")\n",
        "        return translator\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Error loading translation model: {str(e)}\", \"ERROR\")\n",
        "        \n",
        "        # Try with fallback model\n",
        "        try:\n",
        "            logger.log(f\"Attempting to load fallback model: {FALLBACK_MODEL}\")\n",
        "            \n",
        "            # Load tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL)\n",
        "            \n",
        "            # Configure model with more aggressive memory optimizations\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                FALLBACK_MODEL,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                low_cpu_mem_usage=True,\n",
        "                cache_dir=cache_dir\n",
        "            )\n",
        "            \n",
        "            # Create pipeline with smaller batch size\n",
        "            translator = pipeline(\n",
        "                \"translation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                batch_size=BATCH_SIZE // 2  # Smaller batch size for fallback\n",
        "            )\n",
        "            \n",
        "            logger.log(f\"Successfully loaded fallback model: {FALLBACK_MODEL}\")\n",
        "            return translator\n",
        "        except Exception as e:\n",
        "            logger.log(f\"Error loading fallback model: {str(e)}\", \"ERROR\")\n",
        "            logger.log(\"Will use placeholder translations instead\", \"WARNING\")\n",
        "            return None\n",
        "\n",
        "def translate_batch(word_batch, source_lang, target_lang, translator, max_retries=3):\n",
        "    \"\"\"Translate a batch of words with improved error handling and retries\"\"\"\n",
        "    # Normalize and clean input words\n",
        "    normalized_batch = [str(word).lower().strip() for word in word_batch]\n",
        "    \n",
        "    # Attempt translation with retries\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            results = translator(normalized_batch, \n",
        "                                src_lang=source_lang, \n",
        "                                tgt_lang=target_lang,\n",
        "                                max_length=128)\n",
        "            \n",
        "            # Extract translations\n",
        "            translations = [result['translation_text'].strip() for result in results]\n",
        "            return translations, True\n",
        "            \n",
        "        except Exception as e:\n",
        "            if retry == max_retries - 1:  # Last retry\n",
        "                # Final fallback: translate one by one\n",
        "                translations = []\n",
        "                success = False\n",
        "                for word in normalized_batch:\n",
        "                    try:\n",
        "                        result = translator(word, \n",
        "                                         src_lang=source_lang, \n",
        "                                         tgt_lang=target_lang,\n",
        "                                         max_length=128)\n",
        "                        translation = result[0]['translation_text'].strip()\n",
        "                        translations.append(translation)\n",
        "                        success = True\n",
        "                    except:\n",
        "                        # If individual translation fails, use original word\n",
        "                        translations.append(f\"[{target_lang}_{word}]\")\n",
        "                \n",
        "                return translations, success\n",
        "            else:\n",
        "                # Wait before retrying (with exponential backoff)\n",
        "                time.sleep(1 * (2 ** retry))\n",
        "    \n",
        "    # If all retries fail\n",
        "    return [f\"[{target_lang}_{word}]\" for word in normalized_batch], False\n",
        "\n",
        "def translate_pos_category(pos, words_df, translator, logger, cache_manager, translations):\n",
        "    \"\"\"Translate all words for a specific POS category in batches\"\"\"\n",
        "    # Get words for this POS\n",
        "    pos_words = words_df[words_df['pos'] == pos]\n",
        "    word_list = pos_words['word'].tolist()\n",
        "    \n",
        "    # Check which words still need translation\n",
        "    to_translate = []\n",
        "    for word in word_list:\n",
        "        if not all(word in translations.get(lang, {}) for lang in ['french', 'bambara', 'wolof']):\n",
        "            to_translate.append(word)\n",
        "    \n",
        "    if not to_translate:\n",
        "        logger.log(f\"All {pos} words already translated\")\n",
        "        return translations\n",
        "    \n",
        "    logger.log(f\"Translating {len(to_translate)} {pos} words\")\n",
        "    \n",
        "    # Process in batches\n",
        "    batch_size = BATCH_SIZE\n",
        "    batches = [to_translate[i:i+batch_size] for i in range(0, len(to_translate), batch_size)]\n",
        "    \n",
        "    for i, batch in enumerate(tqdm(batches, desc=f\"Translating {pos} batches\")):\n",
        "        batch_translations = {}\n",
        "        \n",
        "        # Translate to each target language\n",
        "        for lang_name, lang_code in LANGUAGE_CODES.items():\n",
        "            if lang_name == 'english':  # Skip source language\n",
        "                continue\n",
        "                \n",
        "            # Check cache first\n",
        "            batch_key = f\"{pos}_batch_{i}_{lang_name}\"\n",
        "            cached_results = cache_manager.get(batch_key, \"translations\")\n",
        "            \n",
        "            if cached_results:\n",
        "                # Use cached results\n",
        "                batch_translations[lang_name] = cached_results\n",
        "                logger.log(f\"Used cached translations for batch {i} to {lang_name}\")\n",
        "            else:\n",
        "                # Translate batch\n",
        "                try:\n",
        "                    results, success = translate_batch(\n",
        "                        batch, \n",
        "                        LANGUAGE_CODES['english'], \n",
        "                        lang_code, \n",
        "                        translator\n",
        "                    )\n",
        "                    \n",
        "                    # Store results\n",
        "                    batch_translations[lang_name] = results\n",
        "                    \n",
        "                    # Cache successful translations\n",
        "                    if success:\n",
        "                        cache_manager.set(batch_key, results, \"translations\")\n",
        "                        \n",
        "                    # Log progress\n",
        "                    logger.log(f\"Translated batch {i+1}/{len(batches)} to {lang_name}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.log(f\"Error translating batch to {lang_name}: {str(e)}\", \"ERROR\")\n",
        "                    # Use placeholder if translation fails\n",
        "                    batch_translations[lang_name] = [f\"[{lang_name}_{word}]\" for word in batch]\n",
        "        \n",
        "        # Update translations dictionary\n",
        "        for idx, word in enumerate(batch):\n",
        "            for lang in batch_translations:\n",
        "                if lang not in translations:\n",
        "                    translations[lang] = {}\n",
        "                translations[lang][word] = batch_translations[lang][idx]\n",
        "        \n",
        "        # Free up memory\n",
        "        if i % 5 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    \n",
        "    return translations\n",
        "\n",
        "def translate_words(words_df, directories, logger, cache_manager):\n",
        "    \"\"\"Translate words to target languages with parallel processing and caching\"\"\"\n",
        "    output_file = os.path.join(directories[\"translations\"], \"quadrilingual_words.csv\")\n",
        "    checkpoint_file = os.path.join(directories[\"checkpoints\"], \"translations_checkpoint.json\")\n",
        "    \n",
        "    # Skip if already completed\n",
        "    if logger.is_completed(\"word_translation\") and os.path.exists(output_file):\n",
        "        logger.log(\"Skipping word translation - already completed\")\n",
        "        return pd.read_csv(output_file)\n",
        "    \n",
        "    # Load high-quality translations for common words\n",
        "    common_words_file = os.path.join(directories[\"data\"], \"common_words_translations.json\")\n",
        "    \n",
        "    # Dictionary for curated high-quality translations\n",
        "    curated_translations = {\n",
        "        'DET': {\n",
        "            'the': {'french': 'le/la', 'bambara': 'o', 'wolof': 'bi/gi'},\n",
        "            'a': {'french': 'un/une', 'bambara': 'dɔ', 'wolof': 'ab'},\n",
        "            'an': {'french': 'un/une', 'bambara': 'dɔ', 'wolof': 'ab'},\n",
        "            'this': {'french': 'ce/cette', 'bambara': 'nin', 'wolof': 'bii'},\n",
        "            'that': {'french': 'ce/cette', 'bambara': 'o', 'wolof': 'boobu'},\n",
        "            'these': {'french': 'ces', 'bambara': 'ninnu', 'wolof': 'yii'},\n",
        "            'those': {'french': 'ceux/celles', 'bambara': 'olu', 'wolof': 'yooyu'},\n",
        "            'my': {'french': 'mon/ma/mes', 'bambara': 'n ka', 'wolof': 'sama'},\n",
        "            'your': {'french': 'ton/ta/tes', 'bambara': 'i ka', 'wolof': 'sa'},\n",
        "            'his': {'french': 'son/sa/ses', 'bambara': 'a ka', 'wolof': 'am'},\n",
        "            'her': {'french': 'son/sa/ses', 'bambara': 'a ka', 'wolof': 'am'},\n",
        "        },\n",
        "        'PRON': {\n",
        "            'i': {'french': 'je', 'bambara': 'n', 'wolof': 'man'},\n",
        "            'you': {'french': 'tu/vous', 'bambara': 'i', 'wolof': 'yaw'},\n",
        "            'he': {'french': 'il', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'she': {'french': 'elle', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'it': {'french': 'il/elle', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'we': {'french': 'nous', 'bambara': 'an', 'wolof': 'nun'},\n",
        "            'they': {'french': 'ils/elles', 'bambara': 'u', 'wolof': 'ñoom'},\n",
        "            'me': {'french': 'me/moi', 'bambara': 'n', 'wolof': 'ma'},\n",
        "            'him': {'french': 'lui', 'bambara': 'a', 'wolof': 'ko'},\n",
        "            'her': {'french': 'elle/lui', 'bambara': 'a', 'wolof': 'ko'},\n",
        "        },\n",
        "        'NOUN': {\n",
        "            'man': {'french': 'homme', 'bambara': 'cɛ', 'wolof': 'góor'},\n",
        "            'woman': {'french': 'femme', 'bambara': 'muso', 'wolof': 'jigéen'},\n",
        "            'child': {'french': 'enfant', 'bambara': 'den', 'wolof': 'xale'},\n",
        "            'house': {'french': 'maison', 'bambara': 'so', 'wolof': 'kër'},\n",
        "            'water': {'french': 'eau', 'bambara': 'ji', 'wolof': 'ndox'},\n",
        "            'food': {'french': 'nourriture', 'bambara': 'dumuni', 'wolof': 'ñam'},\n",
        "            'day': {'french': 'jour', 'bambara': 'don', 'wolof': 'bés'},\n",
        "            'night': {'french': 'nuit', 'bambara': 'su', 'wolof': 'guddi'},\n",
        "            'sun': {'french': 'soleil', 'bambara': 'tere', 'wolof': 'jant'},\n",
        "            'moon': {'french': 'lune', 'bambara': 'kalo', 'wolof': 'weer'},\n",
        "        },\n",
        "        'VERB': {\n",
        "            'go': {'french': 'aller', 'bambara': 'taa', 'wolof': 'dem'},\n",
        "            'come': {'french': 'venir', 'bambara': 'na', 'wolof': 'ñëw'},\n",
        "            'eat': {'french': 'manger', 'bambara': 'dumu', 'wolof': 'lekk'},\n",
        "            'drink': {'french': 'boire', 'bambara': 'min', 'wolof': 'naan'},\n",
        "            'see': {'french': 'voir', 'bambara': 'ye', 'wolof': 'gis'},\n",
        "            'hear': {'french': 'entendre', 'bambara': 'mɛn', 'wolof': 'dégg'},\n",
        "            'speak': {'french': 'parler', 'bambara': 'kuma', 'wolof': 'wax'},\n",
        "            'walk': {'french': 'marcher', 'bambara': 'taama', 'wolof': 'dox'},\n",
        "            'sleep': {'french': 'dormir', 'bambara': 'sunɔgɔ', 'wolof': 'nelaw'},\n",
        "            'work': {'french': 'travailler', 'bambara': 'baara', 'wolof': 'liggéey'},\n",
        "        },\n",
        "        'ADJ': {\n",
        "            'good': {'french': 'bon', 'bambara': 'ɲuman', 'wolof': 'baax'},\n",
        "            'bad': {'french': 'mauvais', 'bambara': 'juguman', 'wolof': 'bon'},\n",
        "            'big': {'french': 'grand', 'bambara': 'bon', 'wolof': 'mag'},\n",
        "            'small': {'french': 'petit', 'bambara': 'fitini', 'wolof': 'ndaw'},\n",
        "            'hot': {'french': 'chaud', 'bambara': 'goni', 'wolof': 'tàng'},\n",
        "            'cold': {'french': 'froid', 'bambara': 'suma', 'wolof': 'sedd'},\n",
        "            'new': {'french': 'nouveau', 'bambara': 'kura', 'wolof': 'bees'},\n",
        "            'old': {'french': 'vieux', 'bambara': 'kɔrɔ', 'wolof': 'màggat'},\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save curated translations for reference\n",
        "    with open(common_words_file, 'w') as f:\n",
        "        json.dump(curated_translations, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    # Check for checkpoint\n",
        "    translations = {}\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        try:\n",
        "            with open(checkpoint_file, 'r') as f:\n",
        "                translations = json.load(f)\n",
        "            logger.log(f\"Loaded translations for {len(translations.get('french', {}))} words from checkpoint\")\n",
        "        except Exception as e:\n",
        "            logger.log(f\"Error loading checkpoint: {str(e)}\", \"ERROR\")\n",
        "            translations = {'french': {}, 'bambara': {}, 'wolof': {}}\n",
        "    else:\n",
        "        translations = {'french': {}, 'bambara': {}, 'wolof': {}}\n",
        "    \n",
        "    # Add curated translations to translation dictionary\n",
        "    for pos, words in curated_translations.items():\n",
        "        for word, trans in words.items():\n",
        "            for lang, value in trans.items():\n",
        "                if lang not in translations:\n",
        "                    translations[lang] = {}\n",
        "                translations[lang][word] = value\n",
        "    \n",
        "    # Setup translation model\n",
        "    translator = setup_translation_model(logger, directories[\"cache\"])\n",
        "    if not translator:\n",
        "        logger.log(\"Using placeholder translations due to model loading failure\", \"WARNING\")\n",
        "    \n",
        "    # Translate each POS category in parallel\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        futures = {}\n",
        "        \n",
        "        for pos in TARGET_POS:\n",
        "            # Only process one POS at a time to avoid memory issues\n",
        "            futures[executor.submit(\n",
        "                translate_pos_category, \n",
        "                pos, \n",
        "                words_df, \n",
        "                translator, \n",
        "                logger, \n",
        "                cache_manager, \n",
        "                translations\n",
        "            )] = pos\n",
        "            \n",
        "            # Wait for completion before starting next POS\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                pos = futures[future]\n",
        "                try:\n",
        "                    translations = future.result()\n",
        "                    logger.log(f\"Completed translation for {pos}\")\n",
        "                    \n",
        "                    # Save checkpoint after each POS\n",
        "                    with open(checkpoint_file, 'w') as f:\n",
        "                        json.dump(translations, f, ensure_ascii=False)\n",
        "                    \n",
        "                    # Generate interim CSV for this POS\n",
        "                    interim_file = os.path.join(directories[\"interim\"], f\"{pos}_translations.csv\")\n",
        "                    \n",
        "                    # Create DataFrame for this POS\n",
        "                    pos_words = words_df[words_df['pos'] == pos]\n",
        "                    rows = []\n",
        "                    \n",
        "                    for _, row in pos_words.iterrows():\n",
        "                        word = row['word']\n",
        "                        item = {\n",
        "                            'english': word,\n",
        "                            'french': translations.get('french', {}).get(word, f\"fr_{word}\"),\n",
        "                            'bambara': translations.get('bambara', {}).get(word, f\"bam_{word}\"),\n",
        "                            'wolof': translations.get('wolof', {}).get(word, f\"wol_{word}\"),\n",
        "                            'pos': pos\n",
        "                        }\n",
        "                        rows.append(item)\n",
        "                    \n",
        "                    # Save interim CSV\n",
        "                    pos_df = pd.DataFrame(rows)\n",
        "                    pos_df.to_csv(interim_file, index=False)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.log(f\"Error translating {pos}: {str(e)}\", \"ERROR\")\n",
        "            \n",
        "            # Clear futures for next POS\n",
        "            futures.clear()\n",
        "            \n",
        "            # Clean up memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    \n",
        "    # Create final quadrilingual DataFrame\n",
        "    logger.log(\"Creating final quadrilingual DataFrame\")\n",
        "    \n",
        "    # Create list of all translations\n",
        "    all_translations = []\n",
        "    \n",
        "    for _, row in words_df.iterrows():\n",
        "        word = row['word']\n",
        "        pos = row['pos']\n",
        "        \n",
        "        item = {\n",
        "            'english': word,\n",
        "            'french': translations.get('french', {}).get(word, f\"fr_{word}\"),\n",
        "            'bambara': translations.get('bambara', {}).get(word, f\"bam_{word}\"),\n",
        "            'wolof': translations.get('wolof', {}).get(word, f\"wol_{word}\"),\n",
        "            'pos': pos\n",
        "        }\n",
        "        all_translations.append(item)\n",
        "    \n",
        "    # Create DataFrame and save\n",
        "    quad_df = pd.DataFrame(all_translations)\n",
        "    quad_df.to_csv(output_file, index=False)\n",
        "    \n",
        "    logger.log(f\"Saved {len(quad_df)} translations to {output_file}\")\n",
        "    logger.complete_step(\"word_translation\")\n",
        "    \n",
        "    return quad_df\n",
        "\n",
        "# ====================\n",
        "# POS-ALIGNED CSV GENERATION\n",
        "# ====================\n",
        "\n",
        "def generate_pos_aligned_csv(quad_df, directories, logger):\n",
        "    \"\"\"Generate POS-aligned CSV file with optimized chunked processing\"\"\"\n",
        "    output_file = os.path.join(directories[\"final\"], \"pos_aligned_quadrilingual.csv\")\n",
        "    \n",
        "    # Skip if already completed\n",
        "    if logger.is_completed(\"pos_aligned_csv\") and os.path.exists(output_file):\n",
        "        logger.log(\"Skipping POS-aligned CSV generation - already completed\")\n",
        "        return\n",
        "    \n",
        "    logger.log(\"Generating POS-aligned CSV file\")\n",
        "    display_system_info()\n",
        "    \n",
        "    # Group words by POS category\n",
        "    pos_dict = {}\n",
        "    for pos in TARGET_POS:\n",
        "        pos_words = quad_df[quad_df[\"pos\"] == pos]\n",
        "        if len(pos_words) > 0:\n",
        "            pos_dict[pos] = {\n",
        "                \"english\": pos_words[\"english\"].tolist(),\n",
        "                \"french\": pos_words[\"french\"].tolist(),\n",
        "                \"bambara\": pos_words[\"bambara\"].tolist(),\n",
        "                \"wolof\": pos_words[\"wolof\"].tolist()\n",
        "            }\n",
        "    \n",
        "    # Get headers\n",
        "    pos_header = []\n",
        "    for pos in pos_dict.keys():\n",
        "        # Each POS category spans 4 columns (one per language)\n",
        "        pos_header.extend([pos, \"\", \"\", \"\"])\n",
        "    \n",
        "    lang_header = []\n",
        "    for _ in pos_dict.keys():\n",
        "        lang_header.extend([\"ENG\", \"FR\", \"BAM\", \"WOL\"])\n",
        "    \n",
        "    # Find maximum number of words in any category\n",
        "    max_words = max([len(words[\"english\"]) for words in pos_dict.values()])\n",
        "    logger.log(f\"Writing {max_words} rows of data to CSV\")\n",
        "    \n",
        "    # Process in chunks\n",
        "    chunk_size = 5000\n",
        "    \n",
        "    def process_chunk(start_idx, end_idx):\n",
        "        \"\"\"Process a chunk of rows for the CSV file\"\"\"\n",
        "        rows = []\n",
        "        for i in range(start_idx, end_idx):\n",
        "            row = []\n",
        "            for pos in pos_dict.keys():\n",
        "                words = pos_dict[pos]\n",
        "                # Add words or empty strings if index out of range\n",
        "                en_word = words[\"english\"][i] if i < len(words[\"english\"]) else \"\"\n",
        "                fr_word = words[\"french\"][i] if i < len(words[\"french\"]) else \"\"\n",
        "                bam_word = words[\"bambara\"][i] if i < len(words[\"bambara\"]) else \"\"\n",
        "                wol_word = words[\"wolof\"][i] if i < len(words[\"wolof\"]) else \"\"\n",
        "                row.extend([en_word, fr_word, bam_word, wol_word])\n",
        "            rows.append(row)\n",
        "        return rows\n",
        "    \n",
        "    # Create CSV file with parallel chunk processing\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        \n",
        "        # Write headers\n",
        "        writer.writerow(pos_header)\n",
        "        writer.writerow(lang_header)\n",
        "        \n",
        "        # Process chunks in parallel\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = []\n",
        "            \n",
        "            # Submit all chunks for processing\n",
        "            for start_idx in range(0, max_words, chunk_size):\n",
        "                end_idx = min(start_idx + chunk_size, max_words)\n",
        "                futures.append(executor.submit(process_chunk, start_idx, end_idx))\n",
        "            \n",
        "            # Process chunks as they complete\n",
        "            for i, future in enumerate(tqdm(concurrent.futures.as_completed(futures), \n",
        "                                         total=len(futures), \n",
        "                                         desc=\"Writing CSV chunks\")):\n",
        "                try:\n",
        "                    # Get rows for this chunk\n",
        "                    rows = future.result()\n",
        "                    \n",
        "                    # Write rows to CSV\n",
        "                    for row in rows:\n",
        "                        writer.writerow(row)\n",
        "                    \n",
        "                    # Log progress\n",
        "                    chunk_idx = i + 1\n",
        "                    logger.track_progress(\"CSV generation\", chunk_idx * chunk_size, max_words, \n",
        "                                       f\"Chunk {chunk_idx}/{len(futures)}\")\n",
        "                    \n",
        "                    # Save sample of this chunk\n",
        "                    if i % 10 == 0:\n",
        "                        sample_file = os.path.join(directories[\"samples\"], f\"chunk_{i}_sample.csv\")\n",
        "                        with open(sample_file, \"w\", newline=\"\", encoding=\"utf-8\") as sample_f:\n",
        "                            sample_writer = csv.writer(sample_f)\n",
        "                            sample_writer.writerow(pos_header)\n",
        "                            sample_writer.writerow(lang_header)\n",
        "                            for row in rows[:100]:  # Save first 100 rows as sample\n",
        "                                sample_writer.writerow(row)\n",
        "                except Exception as e:\n",
        "                    logger.log(f\"Error processing chunk {i}: {str(e)}\", \"ERROR\")\n",
        "    \n",
        "    logger.log(f\"Generated POS-aligned CSV with {max_words} rows\")\n",
        "    logger.complete_step(\"pos_aligned_csv\")\n",
        "\n",
        "# ====================\n",
        "# MAIN PIPELINE\n",
        "# ====================\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the complete pipeline with improved parallelism and Google Drive storage\"\"\"\n",
        "    print(f\"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}\")\n",
        "    print(f\"{Fore.CYAN}STARTING MULTILINGUAL DATASET GENERATION PIPELINE{Style.RESET_ALL}\")\n",
        "    print(f\"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Download and verify NLTK resources first\n",
        "    print(f\"{Fore.CYAN}STEP 0: DOWNLOADING AND VERIFYING NLTK RESOURCES{Style.RESET_ALL}\")\n",
        "    nltk_status = download_nltk_data()\n",
        "    \n",
        "    # Import transformers here after NLTK setup\n",
        "    try:\n",
        "        from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "        print(f\"{Fore.GREEN}✓ Successfully imported transformers library{Style.RESET_ALL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{Fore.RED}✗ Error importing transformers: {str(e)}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.RED}This is critical - cannot continue without transformers library{Style.RESET_ALL}\")\n",
        "        return\n",
        "    \n",
        "    # Set up directories\n",
        "    print(f\"{Fore.CYAN}SETTING UP PROJECT DIRECTORIES{Style.RESET_ALL}\")\n",
        "    base_dir, directories = setup_drive_directories()\n",
        "    \n",
        "    # Set up logger\n",
        "    logger = Logger(directories[\"logs\"])\n",
        "    logger.log(f\"Pipeline started using base directory: {base_dir}\")\n",
        "    \n",
        "    # Set up cache manager\n",
        "    cache_manager = CacheManager(directories[\"cache\"])\n",
        "    \n",
        "    # Import and configure wordnet with proper error handling\n",
        "    global WN_POS_MAP\n",
        "    if nltk_status.get(\"wordnet\", False):\n",
        "        try:\n",
        "            from nltk.corpus import wordnet as wn\n",
        "            WN_POS_MAP = {\n",
        "                wn.NOUN: 'NOUN',\n",
        "                wn.VERB: 'VERB',\n",
        "                wn.ADJ: 'ADJ',\n",
        "                wn.ADV: 'ADV'\n",
        "            }\n",
        "            logger.log(\"Successfully configured WordNet\")\n",
        "        except Exception as e:\n",
        "            logger.log(f\"Error configuring WordNet: {str(e)}\", \"ERROR\")\n",
        "            WN_POS_MAP = {}\n",
        "    else:\n",
        "        logger.log(\"WordNet not available, will use alternative word sources\", \"WARNING\")\n",
        "        WN_POS_MAP = {}\n",
        "    \n",
        "    try:\n",
        "        # STEP 1: Extract words with POS tags\n",
        "        logger.log(\"STEP 1: EXTRACTING WORDS WITH POS TAGS\")\n",
        "        words_df = extract_words_with_pos_tags(directories, logger, cache_manager)\n",
        "        \n",
        "        # Display system info\n",
        "        display_system_info()\n",
        "        \n",
        "        # STEP 2: Translate words\n",
        "        logger.log(\"STEP 2: TRANSLATING WORDS\")\n",
        "        quad_df = translate_words(words_df, directories, logger, cache_manager)\n",
        "        \n",
        "        # Display system info\n",
        "        display_system_info()\n",
        "        \n",
        "        # STEP 3: Generate POS-aligned CSV\n",
        "        logger.log(\"STEP 3: GENERATING POS-ALIGNED CSV\")\n",
        "        generate_pos_aligned_csv(quad_df, directories, logger)\n",
        "        \n",
        "        # Display final system info\n",
        "        display_system_info()\n",
        "        \n",
        "        # Calculate execution time\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        hours, remainder = divmod(execution_time, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        \n",
        "        logger.log(f\"PIPELINE COMPLETED SUCCESSFULLY in {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "        logger.summary()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.log(f\"Pipeline failed: {str(e)}\", \"ERROR\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        logger.log(\"See error log for details\", \"ERROR\")\n",
        "    \n",
        "    print(f\"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}\")\n",
        "    print(f\"{Fore.CYAN}PIPELINE EXECUTION COMPLETED{Style.RESET_ALL}\")\n",
        "    print(f\"{Fore.CYAN}{'=' * 70}{Style.RESET_ALL}\")\n",
        "\n",
        "# Entry point\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "199ce35b59cc4a81a458018d63a7f957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c72d71c87024c9fa926e818506d3568",
            "placeholder": "​",
            "style": "IPY_MODEL_8b6f86da8d294474831d0c9a486ed627",
            "value": "Translated 13100 words total:  26%"
          }
        },
        "29a5f0d89f4c4c3ca3ca705ed6cef77c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee5591395e741e4b0706b5b6fd1ff43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_199ce35b59cc4a81a458018d63a7f957",
              "IPY_MODEL_781843b95bc345089c1b60c252ac3e9b",
              "IPY_MODEL_6984f959666243bd913e6effa32e2090"
            ],
            "layout": "IPY_MODEL_8afeb2f39f614744950d7fb158e27405"
          }
        },
        "6984f959666243bd913e6effa32e2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a5f0d89f4c4c3ca3ca705ed6cef77c",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0db1a91c5e4403978149a54e5f5d8b",
            "value": " 131/500 [8:22:26&lt;38:38:54, 377.06s/it]"
          }
        },
        "781843b95bc345089c1b60c252ac3e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53bd732d21c44e2a2cfba5d8193ab24",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8435e5ecae5940739239f19b9a63186b",
            "value": 131
          }
        },
        "8435e5ecae5940739239f19b9a63186b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afeb2f39f614744950d7fb158e27405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6f86da8d294474831d0c9a486ed627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c72d71c87024c9fa926e818506d3568": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0db1a91c5e4403978149a54e5f5d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d53bd732d21c44e2a2cfba5d8193ab24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
