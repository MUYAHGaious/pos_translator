{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ee5591395e741e4b0706b5b6fd1ff43",
            "199ce35b59cc4a81a458018d63a7f957",
            "781843b95bc345089c1b60c252ac3e9b",
            "6984f959666243bd913e6effa32e2090",
            "8afeb2f39f614744950d7fb158e27405",
            "8c72d71c87024c9fa926e818506d3568",
            "8b6f86da8d294474831d0c9a486ed627",
            "d53bd732d21c44e2a2cfba5d8193ab24",
            "8435e5ecae5940739239f19b9a63186b",
            "29a5f0d89f4c4c3ca3ca705ed6cef77c",
            "cb0db1a91c5e4403978149a54e5f5d8b"
          ]
        },
        "id": "h-aUuaCgcXPE",
        "outputId": "788e2784-1de8-4301-a831-b520bfaddd16"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q nltk pandas transformers tqdm colorama psutil\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import datetime\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.corpus import wordnet as wn\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from google.colab import drive\n",
        "from colorama import init, Fore, Style\n",
        "\n",
        "# Initialize colorama for colored terminal output\n",
        "init()\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "# Make sure all resources are downloaded\n",
        "try:\n",
        "    from nltk.tag import pos_tag\n",
        "    pos_tag(['test'])\n",
        "    print(f\"{Fore.GREEN}✅ NLTK tagger is working{Style.RESET_ALL}\")\n",
        "except LookupError:\n",
        "    print(f\"{Fore.YELLOW}⚠️ NLTK tagger error - downloading additional resources{Style.RESET_ALL}\")\n",
        "    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "    nltk.download('universal_tagset', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"{Fore.RED}⚠️ NLTK error: {str(e)}{Style.RESET_ALL}\")\n",
        "\n",
        "# Mount Google Drive for checkpoints\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/language_processing_checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(f\"{Fore.GREEN}✅ Google Drive mounted. Checkpoints will be saved to Drive.{Style.RESET_ALL}\")\n",
        "except:\n",
        "    checkpoint_dir = \"./checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(f\"{Fore.YELLOW}⚠️ Google Drive not mounted. Checkpoints will be saved locally.{Style.RESET_ALL}\")\n",
        "\n",
        "# Set up directories\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"data/pos\", exist_ok=True)\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "os.makedirs(\"interim_results\", exist_ok=True)  # New directory for interim results\n",
        "\n",
        "# Get current time for logging\n",
        "def get_timestamp():\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Create log file for completed steps\n",
        "completed_log = 'data/log_completed.txt'\n",
        "if not os.path.exists(completed_log):\n",
        "    open(completed_log, 'w').close()\n",
        "    print(f\"{Fore.GREEN}✅ Created log file for tracking completed steps{Style.RESET_ALL}\")\n",
        "\n",
        "def log_completed_step(step_name):\n",
        "    \"\"\"Log a completed step to the log file\"\"\"\n",
        "    with open(completed_log, 'a') as f:\n",
        "        f.write(f\"{step_name},{get_timestamp()}\\n\")\n",
        "    print(f\"{Fore.GREEN}✅ [{get_timestamp()}] Completed step: {step_name}{Style.RESET_ALL}\")\n",
        "\n",
        "def is_step_completed(step_name):\n",
        "    \"\"\"Check if a step is already completed based on the log file\"\"\"\n",
        "    if os.path.exists(completed_log):\n",
        "        with open(completed_log, 'r') as f:\n",
        "            completed_steps = [line.strip().split(',')[0] for line in f.readlines()]\n",
        "        return step_name in completed_steps\n",
        "    return False\n",
        "\n",
        "# Progress tracking log\n",
        "progress_log = 'data/progress_log.txt'\n",
        "def log_progress(message, level=\"INFO\"):\n",
        "    \"\"\"Log progress message with timestamp\"\"\"\n",
        "    timestamp = get_timestamp()\n",
        "    color = Fore.GREEN if level == \"INFO\" else Fore.YELLOW if level == \"WARNING\" else Fore.RED\n",
        "\n",
        "    with open(progress_log, 'a') as f:\n",
        "        f.write(f\"[{timestamp}] {level}: {message}\\n\")\n",
        "\n",
        "    print(f\"{color}[{timestamp}] {level}: {message}{Style.RESET_ALL}\")\n",
        "\n",
        "# System info display\n",
        "def display_system_info():\n",
        "    \"\"\"Display system info for monitoring resources\"\"\"\n",
        "    try:\n",
        "        # Get memory usage\n",
        "        memory = psutil.virtual_memory()\n",
        "        memory_used_gb = memory.used / (1024 ** 3)\n",
        "        memory_total_gb = memory.total / (1024 ** 3)\n",
        "        memory_percent = memory.percent\n",
        "\n",
        "        # Get CPU usage\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "\n",
        "        # Get GPU info if available\n",
        "        gpu_info = \"\"\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "            gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
        "            gpu_info = f\"GPU Memory: {gpu_memory_allocated:.2f}GB allocated / {gpu_memory_reserved:.2f}GB reserved\"\n",
        "\n",
        "        print(f\"{Fore.CYAN}--- System Info ---{Style.RESET_ALL}\")\n",
        "        print(f\"Memory: {memory_used_gb:.2f}GB / {memory_total_gb:.2f}GB ({memory_percent}%)\")\n",
        "        print(f\"CPU Usage: {cpu_percent}%\")\n",
        "        if gpu_info:\n",
        "            print(gpu_info)\n",
        "        print(f\"{Fore.CYAN}------------------{Style.RESET_ALL}\")\n",
        "    except:\n",
        "        print(f\"{Fore.YELLOW}⚠️ Couldn't display system info{Style.RESET_ALL}\")\n",
        "\n",
        "# Configuration\n",
        "MAX_WORDS_PER_POS = 50000  # Maximum words per POS category\n",
        "BATCH_SIZE = 100  # Translation batch size\n",
        "TARGET_POS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'DET', 'PRON', 'ADP', 'CONJ', 'NUM', 'INTJ']\n",
        "INTERIM_SAVE_EVERY = 1000  # Save interim results every 1000 words\n",
        "\n",
        "# Save interim results in chunks of 1000\n",
        "def save_interim_results(data, pos, chunk_number):\n",
        "    \"\"\"Save interim results after each chunk of translations\"\"\"\n",
        "    filename = f\"interim_results/interim_{pos}_chunk_{chunk_number}.csv\"\n",
        "\n",
        "    # Create DataFrame and save\n",
        "    temp_df = pd.DataFrame(data)\n",
        "    temp_df.to_csv(filename, index=False)\n",
        "\n",
        "    total_words = len(data)\n",
        "    log_progress(f\"Saved interim results for {pos}: {total_words} words (chunk {chunk_number})\")\n",
        "\n",
        "    # Also generate a small POS-aligned sample\n",
        "    generate_interim_pos_aligned_sample(data, pos, chunk_number)\n",
        "\n",
        "def generate_interim_pos_aligned_sample(data, pos, chunk_number):\n",
        "    \"\"\"Generate a small POS-aligned sample CSV for the current chunk\"\"\"\n",
        "    filename = f\"interim_results/interim_aligned_{pos}_chunk_{chunk_number}.csv\"\n",
        "\n",
        "    # Convert data to DataFrame\n",
        "    pos_df = pd.DataFrame(data)\n",
        "\n",
        "    # Group by POS\n",
        "    pos_dict = {}\n",
        "    for p in pos_df['pos'].unique():\n",
        "        pos_words = pos_df[pos_df[\"pos\"] == p]\n",
        "        if len(pos_words) > 0:\n",
        "            pos_dict[p] = {\n",
        "                \"english\": pos_words[\"english\"].tolist(),\n",
        "                \"french\": pos_words[\"french\"].tolist(),\n",
        "                \"bambara\": pos_words[\"bambara\"].tolist(),\n",
        "                \"wolof\": pos_words[\"wolof\"].tolist()\n",
        "            }\n",
        "\n",
        "    # Generate a small sample\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # First header row: POS categories\n",
        "        pos_header = []\n",
        "        for p in pos_dict.keys():\n",
        "            # Each POS category spans 4 columns (one per language)\n",
        "            pos_header.extend([p, \"\", \"\", \"\"])\n",
        "        writer.writerow(pos_header)\n",
        "\n",
        "        # Second header row: Language codes\n",
        "        lang_header = []\n",
        "        for _ in pos_dict.keys():\n",
        "            lang_header.extend([\"ENG\", \"FR\", \"BAM\", \"WOL\"])\n",
        "        writer.writerow(lang_header)\n",
        "\n",
        "        # Find maximum number of words for this chunk\n",
        "        max_chunk_words = min(200, max([len(words[\"english\"]) for words in pos_dict.values()]))\n",
        "\n",
        "        # Write data rows\n",
        "        for i in range(max_chunk_words):\n",
        "            row = []\n",
        "            for p in pos_dict.keys():\n",
        "                words = pos_dict[p]\n",
        "                # Add words or empty strings if index out of range\n",
        "                en_word = words[\"english\"][i] if i < len(words[\"english\"]) else \"\"\n",
        "                fr_word = words[\"french\"][i] if i < len(words[\"french\"]) else \"\"\n",
        "                bam_word = words[\"bambara\"][i] if i < len(words[\"bambara\"]) else \"\"\n",
        "                wol_word = words[\"wolof\"][i] if i < len(words[\"wolof\"]) else \"\"\n",
        "\n",
        "                row.extend([en_word, fr_word, bam_word, wol_word])\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "    log_progress(f\"Generated interim aligned sample for {pos} (chunk {chunk_number})\")\n",
        "\n",
        "# WordNet POS mapping\n",
        "WN_POS_MAP = {\n",
        "    wn.NOUN: 'NOUN',\n",
        "    wn.VERB: 'VERB',\n",
        "    wn.ADJ: 'ADJ',\n",
        "    wn.ADV: 'ADV'\n",
        "}\n",
        "\n",
        "# POS tag mapping (for words not in WordNet)\n",
        "TAG_POS_MAP = {\n",
        "    'NN': 'NOUN', 'NNS': 'NOUN', 'NNP': 'NOUN', 'NNPS': 'NOUN',\n",
        "    'VB': 'VERB', 'VBD': 'VERB', 'VBG': 'VERB', 'VBN': 'VERB', 'VBP': 'VERB', 'VBZ': 'VERB',\n",
        "    'JJ': 'ADJ', 'JJR': 'ADJ', 'JJS': 'ADJ',\n",
        "    'RB': 'ADV', 'RBR': 'ADV', 'RBS': 'ADV',\n",
        "    'DT': 'DET', 'PDT': 'DET', 'WDT': 'DET',\n",
        "    'PRP': 'PRON', 'PRP$': 'PRON', 'WP': 'PRON', 'WP$': 'PRON',\n",
        "    'IN': 'ADP',\n",
        "    'CC': 'CONJ',\n",
        "    'CD': 'NUM',\n",
        "    'UH': 'INTJ'\n",
        "}\n",
        "\n",
        "# PHASE 1: EXTRACT LARGE-SCALE WORD LISTS WITH POS TAGS\n",
        "\n",
        "def extract_large_word_lists():\n",
        "    \"\"\"Extract large-scale word lists with POS tags using WordNet and NLTK\"\"\"\n",
        "    output_file = \"data/pos/english_words_large.csv\"\n",
        "    checkpoint_file = f\"{checkpoint_dir}/pos_extraction_large_checkpoint.csv\"\n",
        "\n",
        "    # Skip if already completed\n",
        "    if is_step_completed(\"pos_extraction_large\") and os.path.exists(output_file):\n",
        "        log_progress(\"Skipping large-scale POS extraction - already completed\")\n",
        "        return pd.read_csv(output_file)\n",
        "\n",
        "    # Check if resuming from checkpoint\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        words_df = pd.read_csv(checkpoint_file)\n",
        "        log_progress(f\"Resuming from checkpoint with {len(words_df)} entries\")\n",
        "    else:\n",
        "        # Initialize DataFrame with columns\n",
        "        words_df = pd.DataFrame(columns=[\"word\", \"pos\"])\n",
        "\n",
        "    log_progress(\"Starting large-scale POS extraction...\")\n",
        "\n",
        "    # Track words we've already added\n",
        "    existing_words = set((row['word'], row['pos']) for _, row in words_df.iterrows())\n",
        "    total_words = len(existing_words)\n",
        "    words_per_pos = {}\n",
        "\n",
        "    for pos in TARGET_POS:\n",
        "        words_per_pos[pos] = len(words_df[words_df['pos'] == pos])\n",
        "        log_progress(f\"Current count for {pos}: {words_per_pos[pos]}/{MAX_WORDS_PER_POS} words\")\n",
        "\n",
        "    # Step 1: Extract words from WordNet (covers NOUN, VERB, ADJ, ADV)\n",
        "    if total_words < MAX_WORDS_PER_POS * len(TARGET_POS):\n",
        "        log_progress(\"Extracting words from WordNet...\")\n",
        "        display_system_info()\n",
        "\n",
        "        # Get words from WordNet for each POS\n",
        "        new_rows = []\n",
        "\n",
        "        for wn_pos, target_pos in WN_POS_MAP.items():\n",
        "            if words_per_pos.get(target_pos, 0) >= MAX_WORDS_PER_POS:\n",
        "                log_progress(f\"Skipping {target_pos} - already have {MAX_WORDS_PER_POS} words\")\n",
        "                continue\n",
        "\n",
        "            log_progress(f\"Processing {target_pos} from WordNet\")\n",
        "            synsets = list(wn.all_synsets(wn_pos))\n",
        "            random.shuffle(synsets)  # Randomize to get a diverse set\n",
        "\n",
        "            # Create progress bar\n",
        "            pbar = tqdm(synsets, desc=f\"Processing {target_pos}\")\n",
        "\n",
        "            chunk_count = words_per_pos.get(target_pos, 0) // INTERIM_SAVE_EVERY\n",
        "            chunk_words = []\n",
        "\n",
        "            for synset in pbar:\n",
        "                for lemma in synset.lemma_names():\n",
        "                    # Skip multi-word expressions and non-alphabetic words\n",
        "                    if '_' in lemma or not lemma.isalpha():\n",
        "                        continue\n",
        "\n",
        "                    lemma = lemma.lower()\n",
        "                    if (lemma, target_pos) not in existing_words:\n",
        "                        new_row = {\"word\": lemma, \"pos\": target_pos}\n",
        "                        new_rows.append(new_row)\n",
        "                        chunk_words.append(new_row)\n",
        "                        existing_words.add((lemma, target_pos))\n",
        "\n",
        "                        # Update progress bar description\n",
        "                        if len(new_rows) % 100 == 0:\n",
        "                            pbar.set_description(f\"Processing {target_pos}: {len(new_rows) + words_per_pos.get(target_pos, 0)} words\")\n",
        "\n",
        "                        # Save interim results for this chunk\n",
        "                        if len(chunk_words) >= INTERIM_SAVE_EVERY:\n",
        "                            save_interim_results(chunk_words, target_pos, chunk_count)\n",
        "                            chunk_count += 1\n",
        "                            chunk_words = []\n",
        "\n",
        "                        # Check if we have enough words for this POS\n",
        "                        if len(new_rows) + words_per_pos.get(target_pos, 0) >= MAX_WORDS_PER_POS:\n",
        "                            break\n",
        "\n",
        "                # Check if we have enough words for this POS\n",
        "                if len(new_rows) + words_per_pos.get(target_pos, 0) >= MAX_WORDS_PER_POS:\n",
        "                    break\n",
        "\n",
        "            # Save any remaining words in the current chunk\n",
        "            if chunk_words:\n",
        "                save_interim_results(chunk_words, target_pos, chunk_count)\n",
        "\n",
        "            # Save checkpoint after each POS\n",
        "            if new_rows:\n",
        "                temp_df = pd.DataFrame(new_rows)\n",
        "                current_df = pd.concat([words_df, temp_df], ignore_index=True)\n",
        "                current_df.to_csv(checkpoint_file, index=False)\n",
        "                current_df.to_csv(output_file, index=False)  # Also save to output file for viewing\n",
        "                words_df = current_df\n",
        "                new_rows = []\n",
        "                log_progress(f\"Checkpoint saved with {len(words_df)} entries\")\n",
        "\n",
        "                # Update counts\n",
        "                words_per_pos[target_pos] = len(words_df[words_df['pos'] == pos])\n",
        "                log_progress(f\"Updated count for {target_pos}: {words_per_pos[target_pos]}/{MAX_WORDS_PER_POS} words\")\n",
        "\n",
        "                # Display system info\n",
        "                display_system_info()\n",
        "\n",
        "    # Step 2: Add common words for categories not well-covered by WordNet\n",
        "    # (DET, PRON, ADP, CONJ, NUM, INTJ)\n",
        "\n",
        "    # Common words for these categories\n",
        "    common_words = {\n",
        "        'DET': ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'our', 'their', 'its',\n",
        "                'each', 'every', 'some', 'any', 'no', 'all', 'both', 'either', 'neither', 'many', 'much', 'most', 'few',\n",
        "                'which', 'what', 'whose', 'whichever', 'whatever'],\n",
        "        'PRON': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'who', 'whom', 'which',\n",
        "                'what', 'whose', 'whoever', 'whomever', 'whatever', 'myself', 'yourself', 'himself', 'herself', 'itself',\n",
        "                'ourselves', 'yourselves', 'themselves', 'each', 'other', 'all', 'another', 'some', 'any', 'somebody',\n",
        "                'anyone', 'everybody', 'nobody', 'something', 'anything', 'nothing', 'everything'],\n",
        "        'ADP': ['in', 'on', 'at', 'by', 'with', 'from', 'to', 'for', 'of', 'about', 'against', 'between', 'through',\n",
        "                'during', 'before', 'after', 'above', 'below', 'under', 'over', 'beside', 'behind', 'across', 'into',\n",
        "                'towards', 'onto', 'beyond', 'along', 'amid', 'among', 'around', 'concerning', 'considering', 'despite',\n",
        "                'except', 'inside', 'like', 'near', 'off', 'out', 'outside', 'past', 'regarding', 'round', 'since',\n",
        "                'throughout', 'till', 'until', 'upon', 'within', 'without'],\n",
        "        'CONJ': ['and', 'or', 'but', 'if', 'when', 'than', 'because', 'although', 'since', 'unless', 'while',\n",
        "                'as', 'that', 'whether', 'before', 'after', 'though', 'so', 'till', 'until', 'whereas', 'for',\n",
        "                'nor', 'yet', 'once', 'provided', 'supposing', 'considering', 'even', 'otherwise', 'however'],\n",
        "        'NUM': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'first', 'second',\n",
        "                'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'once', 'twice', 'thrice',\n",
        "                'dozen', 'hundred', 'thousand', 'million', 'billion', 'trillion', 'zero', 'half', 'quarter', 'double',\n",
        "                'triple', 'quadruple', 'many', 'few', 'several', 'numerous', 'countless'],\n",
        "        'INTJ': ['oh', 'wow', 'hey', 'hello', 'hi', 'yes', 'no', 'okay', 'well', 'ah', 'ouch', 'ugh', 'oops',\n",
        "                'thanks', 'sorry', 'please', 'goodbye', 'bye', 'eh', 'hmm', 'er', 'um', 'alas', 'hurray', 'hooray',\n",
        "                'bravo', 'congratulations', 'cheers', 'yay', 'yikes', 'phew', 'whew', 'huh', 'damn', 'darn', 'gosh',\n",
        "                'goodness', 'jeez', 'bingo', 'voila', 'encore', 'oops', 'psst', 'shh', 'whoa', 'ha', 'haha']\n",
        "    }\n",
        "\n",
        "    new_rows = []\n",
        "    for pos, words in common_words.items():\n",
        "        log_progress(f\"Adding common words for {pos}\")\n",
        "\n",
        "        chunk_count = words_per_pos.get(pos, 0) // INTERIM_SAVE_EVERY\n",
        "        chunk_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if (word, pos) not in existing_words:\n",
        "                new_row = {\"word\": word, \"pos\": pos}\n",
        "                new_rows.append(new_row)\n",
        "                chunk_words.append(new_row)\n",
        "                existing_words.add((word, pos))\n",
        "\n",
        "                # Save interim results for this chunk\n",
        "                if len(chunk_words) >= INTERIM_SAVE_EVERY:\n",
        "                    save_interim_results(chunk_words, pos, chunk_count)\n",
        "                    chunk_count += 1\n",
        "                    chunk_words = []\n",
        "\n",
        "        # Save any remaining words in the current chunk\n",
        "        if chunk_words:\n",
        "            save_interim_results(chunk_words, pos, chunk_count)\n",
        "\n",
        "    # Step 3: Use NLTK's list of English words to fill remaining categories\n",
        "    if total_words + len(new_rows) < MAX_WORDS_PER_POS * len(TARGET_POS):\n",
        "        log_progress(\"Adding words from NLTK's word list...\")\n",
        "\n",
        "        from nltk.corpus import words\n",
        "        english_words = words.words()\n",
        "        random.shuffle(english_words)  # Randomize the list\n",
        "\n",
        "        # Use NLTK's POS tagger to get categories\n",
        "        try:\n",
        "            # Use NLTK's POS tagger for the words\n",
        "            log_progress(\"Tagging words with NLTK (this may take a while)...\")\n",
        "            tagged_words = nltk.pos_tag(english_words[:100000])  # Limit to avoid memory issues\n",
        "\n",
        "            # Track chunks for each POS\n",
        "            chunk_counts = {pos: words_per_pos.get(pos, 0) // INTERIM_SAVE_EVERY for pos in TARGET_POS}\n",
        "            chunk_words = {pos: [] for pos in TARGET_POS}\n",
        "\n",
        "            # Create progress bar\n",
        "            pbar = tqdm(tagged_words, desc=\"Processing NLTK words\")\n",
        "\n",
        "            for word, tag in pbar:\n",
        "                # Skip words we already have\n",
        "                if not word.isalpha() or len(word) <= 2:\n",
        "                    continue\n",
        "\n",
        "                word = word.lower()\n",
        "                pos = TAG_POS_MAP.get(tag)\n",
        "\n",
        "                if pos in TARGET_POS and words_per_pos.get(pos, 0) + len([r for r in new_rows if r['pos'] == pos]) < MAX_WORDS_PER_POS:\n",
        "                    if (word, pos) not in existing_words:\n",
        "                        new_row = {\"word\": word, \"pos\": pos}\n",
        "                        new_rows.append(new_row)\n",
        "                        chunk_words[pos].append(new_row)\n",
        "                        existing_words.add((word, pos))\n",
        "\n",
        "                        # Update progress bar description every 100 words\n",
        "                        if len(new_rows) % 100 == 0:\n",
        "                            pbar.set_description(f\"Processing NLTK words: {len(new_rows)} new words\")\n",
        "\n",
        "                        # Check for complete chunks and save interim results\n",
        "                        for p in TARGET_POS:\n",
        "                            if len(chunk_words[p]) >= INTERIM_SAVE_EVERY:\n",
        "                                save_interim_results(chunk_words[p], p, chunk_counts[p])\n",
        "                                chunk_counts[p] += 1\n",
        "                                chunk_words[p] = []\n",
        "\n",
        "                # Save checkpoint every 1000 words\n",
        "                if len(new_rows) >= 1000:\n",
        "                    temp_df = pd.DataFrame(new_rows)\n",
        "                    current_df = pd.concat([words_df, temp_df], ignore_index=True)\n",
        "                    current_df.to_csv(checkpoint_file, index=False)\n",
        "                    current_df.to_csv(output_file, index=False)  # Also save to output file for viewing\n",
        "                    words_df = current_df\n",
        "                    new_rows = []\n",
        "                    log_progress(f\"Checkpoint saved with {len(words_df)} entries\")\n",
        "\n",
        "                    # Update counts\n",
        "                    for pos in TARGET_POS:\n",
        "                        words_per_pos[pos] = len(words_df[words_df['pos'] == pos])\n",
        "                        log_progress(f\"Updated count for {pos}: {words_per_pos[pos]}/{MAX_WORDS_PER_POS} words\")\n",
        "\n",
        "                    # Check if we have enough words\n",
        "                    if all(words_per_pos.get(pos, 0) >= MAX_WORDS_PER_POS for pos in TARGET_POS):\n",
        "                        break\n",
        "\n",
        "                    # Display system info\n",
        "                    display_system_info()\n",
        "\n",
        "            # Save any remaining chunks\n",
        "            for pos in TARGET_POS:\n",
        "                if chunk_words[pos]:\n",
        "                    save_interim_results(chunk_words[pos], pos, chunk_counts[pos])\n",
        "\n",
        "        except Exception as e:\n",
        "            log_progress(f\"Error processing NLTK words: {str(e)}\", \"ERROR\")\n",
        "            log_progress(\"Continuing with WordNet words only\", \"WARNING\")\n",
        "\n",
        "    # Add any remaining rows\n",
        "    if new_rows:\n",
        "        temp_df = pd.DataFrame(new_rows)\n",
        "        words_df = pd.concat([words_df, temp_df], ignore_index=True)\n",
        "\n",
        "    # Fill remaining categories with synthetic data if needed\n",
        "    log_progress(\"Checking for categories that need more words...\")\n",
        "    display_system_info()\n",
        "\n",
        "    for pos in TARGET_POS:\n",
        "        count = len(words_df[words_df['pos'] == pos])\n",
        "        if count < MAX_WORDS_PER_POS:\n",
        "            log_progress(f\"Only {count}/{MAX_WORDS_PER_POS} words for {pos} - adding synthetic words\", \"WARNING\")\n",
        "\n",
        "            # Generate synthetic words for this category\n",
        "            needed = MAX_WORDS_PER_POS - count\n",
        "            log_progress(f\"Generating {needed} synthetic words for {pos}\")\n",
        "\n",
        "            new_rows = []\n",
        "            # Add synthetic words\n",
        "            existing_words = set(words_df[words_df['pos'] == pos]['word'].tolist())\n",
        "\n",
        "            chunk_count = count // INTERIM_SAVE_EVERY\n",
        "            chunk_words = []\n",
        "\n",
        "            # Create progress bar\n",
        "            pbar = tqdm(range(needed), desc=f\"Generating synthetic {pos} words\")\n",
        "\n",
        "            for i in pbar:\n",
        "                # Create synthetic word\n",
        "                if pos == 'NOUN':\n",
        "                    word = f\"noun{i+1}\"\n",
        "                elif pos == 'VERB':\n",
        "                    word = f\"verb{i+1}\"\n",
        "                elif pos == 'ADJ':\n",
        "                    word = f\"adj{i+1}\"\n",
        "                elif pos == 'ADV':\n",
        "                    word = f\"adv{i+1}\"\n",
        "                elif pos == 'DET':\n",
        "                    word = f\"det{i+1}\"\n",
        "                elif pos == 'PRON':\n",
        "                    word = f\"pron{i+1}\"\n",
        "                elif pos == 'ADP':\n",
        "                    word = f\"adp{i+1}\"\n",
        "                elif pos == 'CONJ':\n",
        "                    word = f\"conj{i+1}\"\n",
        "                elif pos == 'NUM':\n",
        "                    word = f\"num{i+1}\"\n",
        "                elif pos == 'INTJ':\n",
        "                    word = f\"intj{i+1}\"\n",
        "                else:\n",
        "                    word = f\"word{pos}{i+1}\"\n",
        "\n",
        "                # Skip if word already exists\n",
        "                if word in existing_words:\n",
        "                    continue\n",
        "\n",
        "                new_row = {\"word\": word, \"pos\": pos}\n",
        "                new_rows.append(new_row)\n",
        "                chunk_words.append(new_row)\n",
        "                existing_words.add(word)\n",
        "\n",
        "                # Update progress bar every 100 words\n",
        "                if (i+1) % 100 == 0:\n",
        "                    pbar.set_description(f\"Generated {i+1}/{needed} synthetic {pos} words\")\n",
        "\n",
        "                # Save interim results for this chunk\n",
        "                if len(chunk_words) >= INTERIM_SAVE_EVERY:\n",
        "                    save_interim_results(chunk_words, pos, chunk_count)\n",
        "                    chunk_count += 1\n",
        "                    chunk_words = []\n",
        "\n",
        "                # Save checkpoint every 10000 words\n",
        "                if len(new_rows) % 10000 == 0:\n",
        "                    temp_df = pd.DataFrame(new_rows)\n",
        "                    current_df = pd.concat([words_df, temp_df], ignore_index=True)\n",
        "                    current_df.to_csv(checkpoint_file, index=False)\n",
        "                    current_df.to_csv(output_file, index=False)  # Also save to output file for viewing\n",
        "                    words_df = current_df\n",
        "                    new_rows = []\n",
        "                    log_progress(f\"Checkpoint saved with {len(words_df)} entries\")\n",
        "\n",
        "                    # Display system info\n",
        "                    display_system_info()\n",
        "\n",
        "            # Save any remaining words in the current chunk\n",
        "            if chunk_words:\n",
        "                save_interim_results(chunk_words, pos, chunk_count)\n",
        "\n",
        "            # Add remaining rows\n",
        "            if new_rows:\n",
        "                temp_df = pd.DataFrame(new_rows)\n",
        "                words_df = pd.concat([words_df, temp_df], ignore_index=True)\n",
        "                log_progress(f\"Added {len(new_rows)} synthetic words for {pos}\")\n",
        "\n",
        "    # Final counts\n",
        "    for pos in TARGET_POS:\n",
        "        count = len(words_df[words_df['pos'] == pos])\n",
        "        log_progress(f\"Final count for {pos}: {count}/{MAX_WORDS_PER_POS} words\")\n",
        "\n",
        "    # Save to file\n",
        "    words_df.to_csv(output_file, index=False)\n",
        "\n",
        "    log_progress(\"Extracted large-scale word lists with POS tags\")\n",
        "    display_system_info()\n",
        "\n",
        "    log_completed_step(\"pos_extraction_large\")\n",
        "\n",
        "    return words_df\n",
        "\n",
        "# PHASE 2: TRANSLATE WORDS EFFICIENTLY\n",
        "\n",
        "def translate_word_batch(word_batch, target_lang, translator):\n",
        "    \"\"\"Translate a batch of words to the target language with improved error handling\"\"\"\n",
        "    translations = []\n",
        "    max_retries = 3\n",
        "\n",
        "    # Added improvement: Normalize input words and ensure all are strings\n",
        "    normalized_batch = [str(word).lower().strip() for word in word_batch]\n",
        "\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            # Use a single API call for the whole batch when possible\n",
        "            if target_lang in ['fra_Latn', 'wol_Latn', 'bam_Latn']:\n",
        "                results = translator(normalized_batch, src_lang=\"eng_Latn\", tgt_lang=target_lang, max_length=200)\n",
        "                translations = [result['translation_text'] for result in results]\n",
        "                break  # Success, exit retry loop\n",
        "            else:\n",
        "                # Fallback to individual translation\n",
        "                translations = []\n",
        "                for word in normalized_batch:\n",
        "                    result = translator(word, src_lang=\"eng_Latn\", tgt_lang=target_lang, max_length=200)\n",
        "                    translations.append(result[0]['translation_text'])\n",
        "                break  # Success, exit retry loop\n",
        "        except Exception as e:\n",
        "            log_progress(f\"Error translating batch (attempt {retry+1}/{max_retries}): {str(e)}\", \"WARNING\")\n",
        "            if retry == max_retries - 1:  # Last retry failed\n",
        "                # Improved fallback: Process words individually with timeouts\n",
        "                translations = []\n",
        "                for word in normalized_batch:\n",
        "                    try:\n",
        "                        # Add timeout between individual word translation attempts\n",
        "                        time.sleep(0.5)\n",
        "                        result = translator(word, src_lang=\"eng_Latn\", tgt_lang=target_lang, max_length=200)\n",
        "                        translations.append(result[0]['translation_text'])\n",
        "                    except:\n",
        "                        translations.append(f\"[ERROR] {word}\")\n",
        "            else:\n",
        "                # Wait before retrying\n",
        "                time.sleep(2 * (retry + 1))  # Exponential backoff\n",
        "\n",
        "    # Post-process translations\n",
        "    clean_translations = []\n",
        "    for trans in translations:\n",
        "        # Remove any extraneous characters that may have been added\n",
        "        clean_trans = trans.strip()\n",
        "        # Handle any common issues in translated text\n",
        "        if clean_trans.startswith(\"[\") and clean_trans.endswith(\"]\"):\n",
        "            clean_trans = clean_trans[1:-1].strip()\n",
        "        clean_translations.append(clean_trans)\n",
        "\n",
        "    return clean_translations\n",
        "\n",
        "def translate_large_word_lists(words_df, use_test_mode=False):\n",
        "    \"\"\"Translate the large word lists to all target languages with improved methods\"\"\"\n",
        "    output_file = \"data/pos/quadrilingual_words_large.csv\"\n",
        "    checkpoint_file = f\"{checkpoint_dir}/word_translation_large_checkpoint.json\"\n",
        "\n",
        "    # Skip if already completed\n",
        "    if is_step_completed(\"word_translation_large\") and os.path.exists(output_file):\n",
        "        log_progress(\"Skipping large-scale word translation - already completed\")\n",
        "        return pd.read_csv(output_file)\n",
        "\n",
        "    # Check if we need test mode\n",
        "    if use_test_mode:\n",
        "        log_progress(\"Using test mode with placeholder translations\")\n",
        "\n",
        "        # Process all words but with placeholder translations\n",
        "        test_data = []\n",
        "\n",
        "        # Process each POS category with samples for testing\n",
        "        for pos in TARGET_POS:\n",
        "            pos_words = words_df[words_df['pos'] == pos]\n",
        "            log_progress(f\"Creating test translations for {len(pos_words)} {pos} words\")\n",
        "\n",
        "            chunk_count = 0\n",
        "            chunk_data = []\n",
        "\n",
        "            # Create progress bar\n",
        "            pbar = tqdm(pos_words.iterrows(), total=len(pos_words), desc=f\"Processing {pos}\")\n",
        "\n",
        "            for i, row in enumerate(pbar):\n",
        "                _, row = row  # Unpack the row\n",
        "                english = row['word']\n",
        "\n",
        "                # Create placeholder translations\n",
        "                french = f\"fr_{english}\"\n",
        "                bambara = f\"bam_{english}\"\n",
        "                wolof = f\"wol_{english}\"\n",
        "\n",
        "                item = {\n",
        "                    'english': english,\n",
        "                    'french': french,\n",
        "                    'bambara': bambara,\n",
        "                    'wolof': wolof,\n",
        "                    'pos': pos\n",
        "                }\n",
        "\n",
        "                test_data.append(item)\n",
        "                chunk_data.append(item)\n",
        "\n",
        "                # Update progress\n",
        "                if (i+1) % 100 == 0:\n",
        "                    pbar.set_description(f\"Processed {i+1}/{len(pos_words)} {pos} words\")\n",
        "\n",
        "                # Save interim results every 1000 words\n",
        "                if len(chunk_data) >= INTERIM_SAVE_EVERY:\n",
        "                    save_interim_results(chunk_data, pos, chunk_count)\n",
        "                    chunk_count += 1\n",
        "                    chunk_data = []\n",
        "\n",
        "                    # Also save progress to CSV periodically\n",
        "                    if len(test_data) % 10000 == 0:\n",
        "                        temp_df = pd.DataFrame(test_data)\n",
        "                        temp_df.to_csv(output_file, index=False)\n",
        "                        log_progress(f\"Saved progress with {len(test_data)} test translations\")\n",
        "\n",
        "                        # Display system info\n",
        "                        display_system_info()\n",
        "\n",
        "            # Save any remaining words in the chunk\n",
        "            if chunk_data:\n",
        "                save_interim_results(chunk_data, pos, chunk_count)\n",
        "\n",
        "        # Create DataFrame and save\n",
        "        test_df = pd.DataFrame(test_data)\n",
        "        test_df.to_csv(output_file, index=False)\n",
        "\n",
        "        log_progress(f\"Created test translations with {len(test_df)} entries\")\n",
        "        log_completed_step(\"word_translation_large\")\n",
        "\n",
        "        return test_df\n",
        "\n",
        "    # Real translation mode\n",
        "    log_progress(\"Translating large word lists to all target languages\")\n",
        "    display_system_info()\n",
        "\n",
        "    # Check if resuming from checkpoint\n",
        "    translations = {}\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            translations = json.load(f)\n",
        "        log_progress(f\"Resuming from checkpoint with {len(translations.get('french', {}))} translated words\")\n",
        "    else:\n",
        "        translations = {'french': {}, 'bambara': {}, 'wolof': {}}\n",
        "\n",
        "    # Initialize translation model\n",
        "    try:\n",
        "        # Set up model with lower precision for memory efficiency\n",
        "        log_progress(\"Loading translation model...\")\n",
        "        model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        )\n",
        "\n",
        "        translator = pipeline(\n",
        "            \"translation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        log_progress(\"Loaded NLLB translation model with memory optimizations\")\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Error loading translation model: {str(e)}\", \"ERROR\")\n",
        "        log_progress(\"Will use placeholder translations\", \"WARNING\")\n",
        "        translator = None\n",
        "\n",
        "    # High-quality translations for common words\n",
        "    common_words = {\n",
        "        'DET': {\n",
        "            'the': {'french': 'le/la', 'bambara': 'o', 'wolof': 'bi/gi'},\n",
        "            'a': {'french': 'un/une', 'bambara': 'dɔ', 'wolof': 'ab'},\n",
        "            'an': {'french': 'un/une', 'bambara': 'dɔ', 'wolof': 'ab'},\n",
        "            'this': {'french': 'ce/cette', 'bambara': 'nin', 'wolof': 'bii'},\n",
        "            'that': {'french': 'ce/cette', 'bambara': 'o', 'wolof': 'boobu'},\n",
        "            'these': {'french': 'ces', 'bambara': 'ninnu', 'wolof': 'yii'},\n",
        "            'those': {'french': 'ceux/celles', 'bambara': 'olu', 'wolof': 'yooyu'},\n",
        "            'my': {'french': 'mon/ma/mes', 'bambara': 'n ka', 'wolof': 'sama'},\n",
        "            'your': {'french': 'ton/ta/tes', 'bambara': 'i ka', 'wolof': 'sa'},\n",
        "            'his': {'french': 'son/sa/ses', 'bambara': 'a ka', 'wolof': 'am'},\n",
        "            'her': {'french': 'son/sa/ses', 'bambara': 'a ka', 'wolof': 'am'},\n",
        "        },\n",
        "        'PRON': {\n",
        "            'i': {'french': 'je', 'bambara': 'n', 'wolof': 'man'},\n",
        "            'you': {'french': 'tu/vous', 'bambara': 'i', 'wolof': 'yaw'},\n",
        "            'he': {'french': 'il', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'she': {'french': 'elle', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'it': {'french': 'il/elle', 'bambara': 'a', 'wolof': 'moom'},\n",
        "            'we': {'french': 'nous', 'bambara': 'an', 'wolof': 'nun'},\n",
        "            'they': {'french': 'ils/elles', 'bambara': 'u', 'wolof': 'ñoom'},\n",
        "            'me': {'french': 'me/moi', 'bambara': 'n', 'wolof': 'ma'},\n",
        "            'him': {'french': 'lui', 'bambara': 'a', 'wolof': 'ko'},\n",
        "            'her': {'french': 'elle/lui', 'bambara': 'a', 'wolof': 'ko'},\n",
        "        },\n",
        "        'NOUN': {\n",
        "            'man': {'french': 'homme', 'bambara': 'cɛ', 'wolof': 'góor'},\n",
        "            'woman': {'french': 'femme', 'bambara': 'muso', 'wolof': 'jigéen'},\n",
        "            'child': {'french': 'enfant', 'bambara': 'den', 'wolof': 'xale'},\n",
        "            'house': {'french': 'maison', 'bambara': 'so', 'wolof': 'kër'},\n",
        "            'water': {'french': 'eau', 'bambara': 'ji', 'wolof': 'ndox'},\n",
        "        },\n",
        "        'VERB': {\n",
        "            'go': {'french': 'aller', 'bambara': 'taa', 'wolof': 'dem'},\n",
        "            'come': {'french': 'venir', 'bambara': 'na', 'wolof': 'ñëw'},\n",
        "            'eat': {'french': 'manger', 'bambara': 'dumu', 'wolof': 'lekk'},\n",
        "            'drink': {'french': 'boire', 'bambara': 'min', 'wolof': 'naan'},\n",
        "            'see': {'french': 'voir', 'bambara': 'ye', 'wolof': 'gis'},\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Process words by POS category\n",
        "    all_translations = []\n",
        "\n",
        "    for pos in TARGET_POS:\n",
        "        pos_words = words_df[words_df['pos'] == pos]\n",
        "        log_progress(f\"Translating {len(pos_words)} {pos} words\")\n",
        "        display_system_info()\n",
        "\n",
        "        chunk_count = 0\n",
        "        chunk_translations = []\n",
        "\n",
        "        # Process in batches\n",
        "        pbar = tqdm(range(0, len(pos_words), BATCH_SIZE), desc=f\"Translating {pos}\")\n",
        "\n",
        "        for i in pbar:\n",
        "            batch = pos_words.iloc[i:i+BATCH_SIZE]\n",
        "            batch_words = batch['word'].tolist()\n",
        "            batch_translations = []\n",
        "\n",
        "            # Only translate words not already in cache\n",
        "            to_translate = []\n",
        "            for word in batch_words:\n",
        "                # Check if it's in our common words dictionary\n",
        "                if pos in common_words and word in common_words[pos]:\n",
        "                    continue\n",
        "                # Check if it's already in our translation cache\n",
        "                if word not in translations['french']:\n",
        "                    to_translate.append(word)\n",
        "\n",
        "            # Translate if needed\n",
        "            if to_translate and translator:\n",
        "                # French translation\n",
        "                log_progress(f\"Translating {len(to_translate)} new words to French\")\n",
        "                fr_translations = translate_word_batch(to_translate, \"fra_Latn\", translator)\n",
        "                for word, trans in zip(to_translate, fr_translations):\n",
        "                    translations['french'][word] = trans\n",
        "\n",
        "                # Bambara translation\n",
        "                log_progress(f\"Translating {len(to_translate)} new words to Bambara\")\n",
        "                bam_translations = translate_word_batch(to_translate, \"bam_Latn\", translator)\n",
        "                for word, trans in zip(to_translate, bam_translations):\n",
        "                    translations['bambara'][word] = trans\n",
        "\n",
        "                # Wolof translation\n",
        "                log_progress(f\"Translating {len(to_translate)} new words to Wolof\")\n",
        "                wol_translations = translate_word_batch(to_translate, \"wol_Latn\", translator)\n",
        "                for word, trans in zip(to_translate, wol_translations):\n",
        "                    translations['wolof'][word] = trans\n",
        "\n",
        "                # Free up memory\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Add translations for this batch\n",
        "            for word in batch_words:\n",
        "                # Check if it's in our common words dictionary first (highest quality)\n",
        "                if pos in common_words and word in common_words[pos]:\n",
        "                    french = common_words[pos][word]['french']\n",
        "                    bambara = common_words[pos][word]['bambara']\n",
        "                    wolof = common_words[pos][word]['wolof']\n",
        "                elif translator:\n",
        "                    # Use cached translations\n",
        "                    french = translations['french'].get(word, f\"[FR] {word}\")\n",
        "                    bambara = translations['bambara'].get(word, f\"[BAM] {word}\")\n",
        "                    wolof = translations['wolof'].get(word, f\"[WOL] {word}\")\n",
        "                else:\n",
        "                    # Use placeholder translations\n",
        "                    french = f\"[FR] {word}\"\n",
        "                    bambara = f\"[BAM] {word}\"\n",
        "                    wolof = f\"[WOL] {word}\"\n",
        "\n",
        "                item = {\n",
        "                    'english': word,\n",
        "                    'french': french,\n",
        "                    'bambara': bambara,\n",
        "                    'wolof': wolof,\n",
        "                    'pos': pos\n",
        "                }\n",
        "\n",
        "                batch_translations.append(item)\n",
        "                chunk_translations.append(item)\n",
        "\n",
        "            # Add to final list\n",
        "            all_translations.extend(batch_translations)\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_description(f\"Translated {len(all_translations)} words total\")\n",
        "\n",
        "            # Save chunk after every 1000 words\n",
        "            if len(chunk_translations) >= INTERIM_SAVE_EVERY:\n",
        "                save_interim_results(chunk_translations, pos, chunk_count)\n",
        "                chunk_count += 1\n",
        "                chunk_translations = []\n",
        "\n",
        "            # Save checkpoint\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                json.dump(translations, f)\n",
        "\n",
        "            # Also save progress to CSV periodically\n",
        "            if len(all_translations) % 10000 == 0:\n",
        "                temp_df = pd.DataFrame(all_translations)\n",
        "                temp_df.to_csv(output_file, index=False)\n",
        "                log_progress(f\"Saved progress with {len(all_translations)} translated words\")\n",
        "\n",
        "                # Display system info\n",
        "                display_system_info()\n",
        "\n",
        "        # Save any remaining words in the chunk\n",
        "        if chunk_translations:\n",
        "            save_interim_results(chunk_translations, pos, chunk_count)\n",
        "\n",
        "        # Save complete POS translations\n",
        "        pos_df = pd.DataFrame([t for t in all_translations if t['pos'] == pos])\n",
        "        pos_df.to_csv(f\"interim_results/complete_{pos}_translations.csv\", index=False)\n",
        "        log_progress(f\"Saved complete translations for {pos}: {len(pos_df)} words\")\n",
        "\n",
        "    # Create final DataFrame\n",
        "    quad_df = pd.DataFrame(all_translations)\n",
        "\n",
        "    # Save to file\n",
        "    quad_df.to_csv(output_file, index=False)\n",
        "\n",
        "    log_progress(f\"Translated {len(quad_df)} words to all languages\")\n",
        "    display_system_info()\n",
        "\n",
        "    log_completed_step(\"word_translation_large\")\n",
        "\n",
        "    return quad_df\n",
        "\n",
        "# PHASE 3: GENERATE POS-ALIGNED CSV\n",
        "\n",
        "def generate_large_pos_aligned_csv(quad_df):\n",
        "    \"\"\"Generate the final large-scale POS-aligned CSV file\"\"\"\n",
        "    output_file = \"output/pos_aligned_quadrilingual_large.csv\"\n",
        "\n",
        "    # Skip if already completed\n",
        "    if is_step_completed(\"csv_generation_large\") and os.path.exists(output_file):\n",
        "        log_progress(\"Skipping large-scale CSV generation - already completed\")\n",
        "        return\n",
        "\n",
        "    log_progress(\"Generating large-scale POS-aligned CSV file\")\n",
        "    display_system_info()\n",
        "\n",
        "    # Group words by POS category\n",
        "    pos_dict = {}\n",
        "    for pos in TARGET_POS:\n",
        "        pos_words = quad_df[quad_df[\"pos\"] == pos]\n",
        "        if len(pos_words) > 0:\n",
        "            pos_dict[pos] = {\n",
        "                \"english\": pos_words[\"english\"].tolist(),\n",
        "                \"french\": pos_words[\"french\"].tolist(),\n",
        "                \"bambara\": pos_words[\"bambara\"].tolist(),\n",
        "                \"wolof\": pos_words[\"wolof\"].tolist()\n",
        "            }\n",
        "\n",
        "    # Create CSV file with the specified format - process in chunks to handle large size\n",
        "    log_progress(\"Writing CSV file (this may take some time for large datasets)\")\n",
        "\n",
        "    # Use streaming approach for large file\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # First header row: POS categories\n",
        "        pos_header = []\n",
        "        for pos in pos_dict.keys():\n",
        "            # Each POS category spans 4 columns (one per language)\n",
        "            pos_header.extend([pos, \"\", \"\", \"\"])\n",
        "        writer.writerow(pos_header)\n",
        "\n",
        "        # Second header row: Language codes\n",
        "        lang_header = []\n",
        "        for _ in pos_dict.keys():\n",
        "            lang_header.extend([\"ENG\", \"FR\", \"BAM\", \"WOL\"])\n",
        "        writer.writerow(lang_header)\n",
        "\n",
        "        # Find maximum number of words in any category\n",
        "        max_words = max([len(words[\"english\"]) for words in pos_dict.values()])\n",
        "        log_progress(f\"Preparing to write {max_words} rows of data\")\n",
        "\n",
        "        # Write data rows in batches\n",
        "        batch_size = 1000\n",
        "        pbar = tqdm(range(0, max_words, batch_size), desc=\"Writing CSV\")\n",
        "\n",
        "        for start_idx in pbar:\n",
        "            end_idx = min(start_idx + batch_size, max_words)\n",
        "\n",
        "            # Create interim aligned sample for this chunk\n",
        "            chunk_rows = []\n",
        "\n",
        "            for i in range(start_idx, end_idx):\n",
        "                row = []\n",
        "                for pos in pos_dict.keys():\n",
        "                    words = pos_dict[pos]\n",
        "                    # Add words or empty strings if index out of range\n",
        "                    en_word = words[\"english\"][i] if i < len(words[\"english\"]) else \"\"\n",
        "                    fr_word = words[\"french\"][i] if i < len(words[\"french\"]) else \"\"\n",
        "                    bam_word = words[\"bambara\"][i] if i < len(words[\"bambara\"]) else \"\"\n",
        "                    wol_word = words[\"wolof\"][i] if i < len(words[\"wolof\"]) else \"\"\n",
        "\n",
        "                    row.extend([en_word, fr_word, bam_word, wol_word])\n",
        "\n",
        "                writer.writerow(row)\n",
        "                chunk_rows.append(row)\n",
        "\n",
        "            # Save an interim version of this chunk\n",
        "            chunk_number = start_idx // batch_size\n",
        "            interim_file = f\"interim_results/aligned_chunk_{chunk_number}.csv\"\n",
        "\n",
        "            with open(interim_file, \"w\", newline=\"\", encoding=\"utf-8\") as chunk_f:\n",
        "                chunk_writer = csv.writer(chunk_f)\n",
        "                chunk_writer.writerow(pos_header)\n",
        "                chunk_writer.writerow(lang_header)\n",
        "                for row in chunk_rows:\n",
        "                    chunk_writer.writerow(row)\n",
        "\n",
        "            log_progress(f\"Saved interim aligned CSV chunk {chunk_number} ({start_idx}-{end_idx-1})\")\n",
        "\n",
        "            # Update progress\n",
        "            progress_pct = min(100, 100 * end_idx / max_words)\n",
        "            pbar.set_description(f\"Writing CSV: {progress_pct:.1f}% complete\")\n",
        "\n",
        "            # Display system info occasionally\n",
        "            if chunk_number % 10 == 0:\n",
        "                display_system_info()\n",
        "\n",
        "    log_progress(f\"Generated large-scale POS-aligned CSV with {max_words} rows\")\n",
        "    display_system_info()\n",
        "\n",
        "    log_completed_step(\"csv_generation_large\")\n",
        "\n",
        "# MAIN EXECUTION\n",
        "\n",
        "def main():\n",
        "    log_progress(\"=\" * 50)\n",
        "    log_progress(\"STARTING LARGE-SCALE POS-ALIGNED CSV GENERATION PIPELINE\")\n",
        "    log_progress(\"=\" * 50)\n",
        "\n",
        "    # Display initial system info\n",
        "    display_system_info()\n",
        "\n",
        "    # Step 1: Extract large-scale word lists with POS tags\n",
        "    log_progress(\"PHASE 1: EXTRACT LARGE-SCALE WORD LISTS WITH POS TAGS\")\n",
        "    words_df = extract_large_word_lists()\n",
        "\n",
        "    # Step 2: Translate words to all target languages\n",
        "    log_progress(\"PHASE 2: TRANSLATE WORDS TO ALL TARGET LANGUAGES\")\n",
        "    quad_df = translate_large_word_lists(words_df, use_test_mode=False)  # Set to False for real translation\n",
        "\n",
        "    # Step 3: Generate large-scale POS-aligned CSV\n",
        "    log_progress(\"PHASE 3: GENERATE LARGE-SCALE POS-ALIGNED CSV\")\n",
        "    generate_large_pos_aligned_csv(quad_df)\n",
        "\n",
        "    log_progress(\"=\" * 50)\n",
        "    log_progress(\"🎉 LARGE-SCALE PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    log_progress(\"=\" * 50)\n",
        "\n",
        "    # Display final system info\n",
        "    display_system_info()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "199ce35b59cc4a81a458018d63a7f957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c72d71c87024c9fa926e818506d3568",
            "placeholder": "​",
            "style": "IPY_MODEL_8b6f86da8d294474831d0c9a486ed627",
            "value": "Translated 13100 words total:  26%"
          }
        },
        "29a5f0d89f4c4c3ca3ca705ed6cef77c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee5591395e741e4b0706b5b6fd1ff43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_199ce35b59cc4a81a458018d63a7f957",
              "IPY_MODEL_781843b95bc345089c1b60c252ac3e9b",
              "IPY_MODEL_6984f959666243bd913e6effa32e2090"
            ],
            "layout": "IPY_MODEL_8afeb2f39f614744950d7fb158e27405"
          }
        },
        "6984f959666243bd913e6effa32e2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a5f0d89f4c4c3ca3ca705ed6cef77c",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0db1a91c5e4403978149a54e5f5d8b",
            "value": " 131/500 [8:22:26&lt;38:38:54, 377.06s/it]"
          }
        },
        "781843b95bc345089c1b60c252ac3e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53bd732d21c44e2a2cfba5d8193ab24",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8435e5ecae5940739239f19b9a63186b",
            "value": 131
          }
        },
        "8435e5ecae5940739239f19b9a63186b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afeb2f39f614744950d7fb158e27405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6f86da8d294474831d0c9a486ed627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c72d71c87024c9fa926e818506d3568": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0db1a91c5e4403978149a54e5f5d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d53bd732d21c44e2a2cfba5d8193ab24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
